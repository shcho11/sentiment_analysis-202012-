{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ENG_SentimentAnalysis_2020511038.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shcho11/sentiment_analysis_202012/blob/main/ENG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9TOC7HKrcu-"
      },
      "source": [
        "# 영어 감정분석기\n",
        "<br>\n",
        "제작자: 조송현 <br>\n",
        "소속: 컴퓨터정보통신대학원 빅데이터융합학과<br>\n",
        "학번 : 2020511038<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qFdeQTFr8l2"
      },
      "source": [
        "## 1. 전처리 <br>\n",
        "## 1) 어절분리\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhlrf7WWt6De"
      },
      "source": [
        "1) Rachel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC3rSTSjuGoo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b65c96e9-9a60-4ba5-d527-6263935a7d67"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentences=\"Y'know, ever since I ran out on Barry at the wedding, I have wondered whether I made the right choice.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"I just went to your building and you weren't there and then this guy with a big hammer said you might be here and you are, you are!\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"Ooh! Look! Look! Look! Look, there's Joey's picture! This is so exciting!\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"Ahh, yes, I will have a glass of the Merlot and uh,  he will have a white wine spritzer.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"Hey, time-out, umm, yeah, does the captain know that we?re moving?\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"I think, if it was a little colder in here I could see your nipples through that sweater.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"Hey Mon, what are you doing now? Wanna come see a movie with us?\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"Are you kidding? I'm trained for nothing! I was laughed out of twelve interviews today.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"You would be too if you found John and David boots on sale, fifty percent off!\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"They're my new 'I don't need a job, I don't need my parents, I've got great boots' boots!\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"Well hello! Welcome to Monica's. May I take your coat?\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"No, no, not at the moment, no, I'm not. Are you?\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"Right, yeah, I've heard that about cute doctors.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"Well, actually Gunther sent me. You?re not allowed to have cups out here, it?s a thing.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[\"Y'know\", ',', 'ever', 'since', 'I', 'ran', 'out', 'on', 'Barry', 'at', 'the', 'wedding', ',', 'I', 'have', 'wondered', 'whether', 'I', 'made', 'the', 'right', 'choice', '.']\n",
            "['I', 'just', 'went', 'to', 'your', 'building', 'and', 'you', 'were', \"n't\", 'there', 'and', 'then', 'this', 'guy', 'with', 'a', 'big', 'hammer', 'said', 'you', 'might', 'be', 'here', 'and', 'you', 'are', ',', 'you', 'are', '!']\n",
            "['Ooh', '!', 'Look', '!', 'Look', '!', 'Look', '!', 'Look', ',', 'there', \"'s\", 'Joey', \"'s\", 'picture', '!', 'This', 'is', 'so', 'exciting', '!']\n",
            "['Ahh', ',', 'yes', ',', 'I', 'will', 'have', 'a', 'glass', 'of', 'the', 'Merlot', 'and', 'uh', ',', 'he', 'will', 'have', 'a', 'white', 'wine', 'spritzer', '.']\n",
            "['Hey', ',', 'time-out', ',', 'umm', ',', 'yeah', ',', 'does', 'the', 'captain', 'know', 'that', 'we', '?', 're', 'moving', '?']\n",
            "['I', 'think', ',', 'if', 'it', 'was', 'a', 'little', 'colder', 'in', 'here', 'I', 'could', 'see', 'your', 'nipples', 'through', 'that', 'sweater', '.']\n",
            "['Hey', 'Mon', ',', 'what', 'are', 'you', 'doing', 'now', '?', 'Wan', 'na', 'come', 'see', 'a', 'movie', 'with', 'us', '?']\n",
            "['Are', 'you', 'kidding', '?', 'I', \"'m\", 'trained', 'for', 'nothing', '!', 'I', 'was', 'laughed', 'out', 'of', 'twelve', 'interviews', 'today', '.']\n",
            "['You', 'would', 'be', 'too', 'if', 'you', 'found', 'John', 'and', 'David', 'boots', 'on', 'sale', ',', 'fifty', 'percent', 'off', '!']\n",
            "['They', \"'re\", 'my', 'new', \"'I\", 'do', \"n't\", 'need', 'a', 'job', ',', 'I', 'do', \"n't\", 'need', 'my', 'parents', ',', 'I', \"'ve\", 'got', 'great', 'boots', \"'\", 'boots', '!']\n",
            "['Well', 'hello', '!', 'Welcome', 'to', 'Monica', \"'s\", '.', 'May', 'I', 'take', 'your', 'coat', '?']\n",
            "['No', ',', 'no', ',', 'not', 'at', 'the', 'moment', ',', 'no', ',', 'I', \"'m\", 'not', '.', 'Are', 'you', '?']\n",
            "['Right', ',', 'yeah', ',', 'I', \"'ve\", 'heard', 'that', 'about', 'cute', 'doctors', '.']\n",
            "['Well', ',', 'actually', 'Gunther', 'sent', 'me', '.', 'You', '?', 're', 'not', 'allowed', 'to', 'have', 'cups', 'out', 'here', ',', 'it', '?', 's', 'a', 'thing', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CuCWZAshI0J"
      },
      "source": [
        "2) Monica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjolYfDOhQ3L",
        "outputId": "7e7bea9e-20f5-4307-eb68-3aa6f6038cf7"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "sentences=\"Come on.  Hello?  I?m sorry you have the wrong number.   Okay, I?ll call you later dad. I love you.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Hello? No rejection? I got shot down at fat camp! Boy, kids are mean when they?re hungry.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"I mean, what are you gonna do, never going to talk to her again?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"I mean I know it?s weird, it?s awkward, but you gotta at least try.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Okay, everybody, this is Rachel, another Lincoln High survivor.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"This is everybody, this is Chandler, and Phoebe, and Joey, and- you remember my brother Ross?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Probably some y'know, European good-bye thing he picked up in London.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Well, the end table is wrong, The couch looks bizarre and don't even get me started on the refrigerator magnets.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"She?s a hooker! She?s a hooker! She?s a?  Hi! Uh, we spoke on the phone.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"I know this is going to sound unbelievably selfish, but, were you planning on bringing up the whole baby/lesbian thing?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Hey guys! Do you wanna look at the song list for the wedding?  Guys?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"I?m sorry. I?m sorry. I-I should probably leave you girls alone.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Hey Frannie, welcome back! How was Florida?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"You mean you know Paul like I know Paul?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['Come', 'on', '.', 'Hello', '?', 'I', '?', 'm', 'sorry', 'you', 'have', 'the', 'wrong', 'number', '.', 'Okay', ',', 'I', '?', 'll', 'call', 'you', 'later', 'dad', '.', 'I', 'love', 'you', '.']\n",
            "['Hello', '?', 'No', 'rejection', '?', 'I', 'got', 'shot', 'down', 'at', 'fat', 'camp', '!', 'Boy', ',', 'kids', 'are', 'mean', 'when', 'they', '?', 're', 'hungry', '.']\n",
            "['I', 'mean', ',', 'what', 'are', 'you', 'gon', 'na', 'do', ',', 'never', 'going', 'to', 'talk', 'to', 'her', 'again', '?']\n",
            "['I', 'mean', 'I', 'know', 'it', '?', 's', 'weird', ',', 'it', '?', 's', 'awkward', ',', 'but', 'you', 'got', 'ta', 'at', 'least', 'try', '.']\n",
            "['Okay', ',', 'everybody', ',', 'this', 'is', 'Rachel', ',', 'another', 'Lincoln', 'High', 'survivor', '.']\n",
            "['This', 'is', 'everybody', ',', 'this', 'is', 'Chandler', ',', 'and', 'Phoebe', ',', 'and', 'Joey', ',', 'and-', 'you', 'remember', 'my', 'brother', 'Ross', '?']\n",
            "['Probably', 'some', \"y'know\", ',', 'European', 'good-bye', 'thing', 'he', 'picked', 'up', 'in', 'London', '.']\n",
            "['Well', ',', 'the', 'end', 'table', 'is', 'wrong', ',', 'The', 'couch', 'looks', 'bizarre', 'and', 'do', \"n't\", 'even', 'get', 'me', 'started', 'on', 'the', 'refrigerator', 'magnets', '.']\n",
            "['She', '?', 's', 'a', 'hooker', '!', 'She', '?', 's', 'a', 'hooker', '!', 'She', '?', 's', 'a', '?', 'Hi', '!', 'Uh', ',', 'we', 'spoke', 'on', 'the', 'phone', '.']\n",
            "['I', 'know', 'this', 'is', 'going', 'to', 'sound', 'unbelievably', 'selfish', ',', 'but', ',', 'were', 'you', 'planning', 'on', 'bringing', 'up', 'the', 'whole', 'baby/lesbian', 'thing', '?']\n",
            "['Hey', 'guys', '!', 'Do', 'you', 'wan', 'na', 'look', 'at', 'the', 'song', 'list', 'for', 'the', 'wedding', '?', 'Guys', '?']\n",
            "['I', '?', 'm', 'sorry', '.', 'I', '?', 'm', 'sorry', '.', 'I-I', 'should', 'probably', 'leave', 'you', 'girls', 'alone', '.']\n",
            "['Hey', 'Frannie', ',', 'welcome', 'back', '!', 'How', 'was', 'Florida', '?']\n",
            "['You', 'mean', 'you', 'know', 'Paul', 'like', 'I', 'know', 'Paul', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHR0rfF9if-E"
      },
      "source": [
        "3) Chandler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHY-GsGdivC3",
        "outputId": "34ebe361-0c00-4d8f-902a-aa28da0b7620"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "sentences=\"I don?t want him to tell this story for years.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"All right! Go left! Go left! Go right!! Go right!!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"So what do you say, maybe sometime I hold your gun?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"You can always spot someone who's never seen one of his plays before. Notice, no fear, no sense of impending doom...\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"I got caught up and work, but I'm quitting tomorrow.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Oh, why not. Was I doing anything particularly... saucy?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"No, I don't see anything different other than the fact that the room got so much brighter when you came into it.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Yeah, I miss that too. I tell you what; from now on we?ll make time to hang out with each other.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"I really shouldn?t have said that you were embarrassing me, I mean that really wasn?t cool.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"And if it makes you feel any better, I?ve had a really lousy day.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Oh, y'know, what did you mean when you said pivot?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Yes, the moon, the glow, the magical feeling, you did this part- Could I get some painkillers over here, please?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['I', 'don', '?', 't', 'want', 'him', 'to', 'tell', 'this', 'story', 'for', 'years', '.']\n",
            "['All', 'right', '!', 'Go', 'left', '!', 'Go', 'left', '!', 'Go', 'right', '!', '!', 'Go', 'right', '!', '!']\n",
            "['So', 'what', 'do', 'you', 'say', ',', 'maybe', 'sometime', 'I', 'hold', 'your', 'gun', '?']\n",
            "['You', 'can', 'always', 'spot', 'someone', 'who', \"'s\", 'never', 'seen', 'one', 'of', 'his', 'plays', 'before', '.', 'Notice', ',', 'no', 'fear', ',', 'no', 'sense', 'of', 'impending', 'doom', '...']\n",
            "['I', 'got', 'caught', 'up', 'and', 'work', ',', 'but', 'I', \"'m\", 'quitting', 'tomorrow', '.']\n",
            "['Oh', ',', 'why', 'not', '.', 'Was', 'I', 'doing', 'anything', 'particularly', '...', 'saucy', '?']\n",
            "['No', ',', 'I', 'do', \"n't\", 'see', 'anything', 'different', 'other', 'than', 'the', 'fact', 'that', 'the', 'room', 'got', 'so', 'much', 'brighter', 'when', 'you', 'came', 'into', 'it', '.']\n",
            "['Yeah', ',', 'I', 'miss', 'that', 'too', '.', 'I', 'tell', 'you', 'what', ';', 'from', 'now', 'on', 'we', '?', 'll', 'make', 'time', 'to', 'hang', 'out', 'with', 'each', 'other', '.']\n",
            "['I', 'really', 'shouldn', '?', 't', 'have', 'said', 'that', 'you', 'were', 'embarrassing', 'me', ',', 'I', 'mean', 'that', 'really', 'wasn', '?', 't', 'cool', '.']\n",
            "['And', 'if', 'it', 'makes', 'you', 'feel', 'any', 'better', ',', 'I', '?', 've', 'had', 'a', 'really', 'lousy', 'day', '.']\n",
            "['Oh', ',', \"y'know\", ',', 'what', 'did', 'you', 'mean', 'when', 'you', 'said', 'pivot', '?']\n",
            "['Yes', ',', 'the', 'moon', ',', 'the', 'glow', ',', 'the', 'magical', 'feeling', ',', 'you', 'did', 'this', 'part-', 'Could', 'I', 'get', 'some', 'painkillers', 'over', 'here', ',', 'please', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkFCLtjzjtqh"
      },
      "source": [
        "4) Joey"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVcb-dHAjvzU",
        "outputId": "5370dc4f-698d-4b00-92fc-d0323b162e1d"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "sentences=\"Then you gotta come clean with Ma! This is not right!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Listen, uh, what do you say I buy you that cup of coffee now?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Whoa-whoa! No-no-no-no-no, nothing is going up! Okay? Up, up is not an option?what's a urethra?  Are you crazy?!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Jo?s there, but I don?t think there?s anything she could do.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Oh thanks. Thanks. It was great meetin? ya. And listen if any of my friends gets married, or have a birthday, or a Tuesday\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Well, Chandler's old roomate was Jewish, and these are the only candles we have, so... Happy Chanukah, everyone.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Vell, Eva, ve've done some excellent vork here, and I vould have to say, your pwoblem is qviiite clear.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Oh sure?And hey, don?t get me wrong, I am so happy for you guys. I just?I miss?hanging out?just-just us, y?know?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"He seemed like a stand up guy. Oh, and he?s not into anything weird sexually.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"All right well, y?know?I guess we know what we have to do to get down.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Ross. I was thinking we could just go down the fire escape.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Noo!! I?ve had the best day ever! Dude, check this out!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Great story!? But, I uh, I gotta go, I got a date with Andrea--Angela--Andrea...? Oh man,\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['Then', 'you', 'got', 'ta', 'come', 'clean', 'with', 'Ma', '!', 'This', 'is', 'not', 'right', '!']\n",
            "['Listen', ',', 'uh', ',', 'what', 'do', 'you', 'say', 'I', 'buy', 'you', 'that', 'cup', 'of', 'coffee', 'now', '?']\n",
            "['Whoa-whoa', '!', 'No-no-no-no-no', ',', 'nothing', 'is', 'going', 'up', '!', 'Okay', '?', 'Up', ',', 'up', 'is', 'not', 'an', 'option', '?', 'what', \"'s\", 'a', 'urethra', '?', 'Are', 'you', 'crazy', '?', '!']\n",
            "['Jo', '?', 's', 'there', ',', 'but', 'I', 'don', '?', 't', 'think', 'there', '?', 's', 'anything', 'she', 'could', 'do', '.']\n",
            "['Oh', 'thanks', '.', 'Thanks', '.', 'It', 'was', 'great', 'meetin', '?', 'ya', '.', 'And', 'listen', 'if', 'any', 'of', 'my', 'friends', 'gets', 'married', ',', 'or', 'have', 'a', 'birthday', ',', 'or', 'a', 'Tuesday']\n",
            "['Well', ',', 'Chandler', \"'s\", 'old', 'roomate', 'was', 'Jewish', ',', 'and', 'these', 'are', 'the', 'only', 'candles', 'we', 'have', ',', 'so', '...', 'Happy', 'Chanukah', ',', 'everyone', '.']\n",
            "['Vell', ',', 'Eva', ',', 've', \"'ve\", 'done', 'some', 'excellent', 'vork', 'here', ',', 'and', 'I', 'vould', 'have', 'to', 'say', ',', 'your', 'pwoblem', 'is', 'qviiite', 'clear', '.']\n",
            "['Oh', 'sure', '?', 'And', 'hey', ',', 'don', '?', 't', 'get', 'me', 'wrong', ',', 'I', 'am', 'so', 'happy', 'for', 'you', 'guys', '.', 'I', 'just', '?', 'I', 'miss', '?', 'hanging', 'out', '?', 'just-just', 'us', ',', 'y', '?', 'know', '?']\n",
            "['He', 'seemed', 'like', 'a', 'stand', 'up', 'guy', '.', 'Oh', ',', 'and', 'he', '?', 's', 'not', 'into', 'anything', 'weird', 'sexually', '.']\n",
            "['All', 'right', 'well', ',', 'y', '?', 'know', '?', 'I', 'guess', 'we', 'know', 'what', 'we', 'have', 'to', 'do', 'to', 'get', 'down', '.']\n",
            "['Ross', '.', 'I', 'was', 'thinking', 'we', 'could', 'just', 'go', 'down', 'the', 'fire', 'escape', '.']\n",
            "['Noo', '!', '!', 'I', '?', 've', 'had', 'the', 'best', 'day', 'ever', '!', 'Dude', ',', 'check', 'this', 'out', '!']\n",
            "['Great', 'story', '!', '?', 'But', ',', 'I', 'uh', ',', 'I', 'got', 'ta', 'go', ',', 'I', 'got', 'a', 'date', 'with', 'Andrea', '--', 'Angela', '--', 'Andrea', '...', '?', 'Oh', 'man', ',']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0xuisF6nJQx"
      },
      "source": [
        "5) Ross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WwlMcQqnNqY",
        "outputId": "209bb0f5-cb34-481f-ff7f-86a8bf80b974"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "sentences=\"Oh, but he will. He still tells the story how Monica tried to escape from fat camp.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Okay, Andre should be there in like 45 minutes. All rightie, bye bye.  Just easier that way.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"The data we are receiving from MRI scans and DNA testing of these fossils are - are staggering.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"I mean, we've been accepting Leakey's dates as a given, but if they're off by even a hundred thousand years or so then you can - you can just throw most of our assumptions, you know, right in the trash.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"So-so what I am saying is - is is that  is that the repercussions could be huge!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"I mean, not just in palaeontology, but if-if you think about it, in evolutionary biology, uh, genetics, geology, uh, I mean, truly the mind boggles!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"You're right, you're right, it is...So you gonna invite us all to the big opening?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Oh, we were helping Chandler write his vows, but he kicked us out because Joey kept making inappropriate suggestions.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Yeah, laugh all you want but in ten minutes we?re gonna have younger looking skin!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Yeah, I guess we don?t have a choice.  Help us! Please help us! We?re stuck up on the roof and we can?t get down!!!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"I know, I wasn?t finished.   But don?t worry! We?re gonna go down the fire escape!!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Y'know, here's the thing. Even if I could get it together enough to- to ask a woman out,... who am I gonna ask?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Okay, and oh I?m gonna need a bunch of extra keys. Apparently I give them away for no reason at all.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"I remember the moonlight coming through the window- and her face had the most incredible glow.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['Oh', ',', 'but', 'he', 'will', '.', 'He', 'still', 'tells', 'the', 'story', 'how', 'Monica', 'tried', 'to', 'escape', 'from', 'fat', 'camp', '.']\n",
            "['Okay', ',', 'Andre', 'should', 'be', 'there', 'in', 'like', '45', 'minutes', '.', 'All', 'rightie', ',', 'bye', 'bye', '.', 'Just', 'easier', 'that', 'way', '.']\n",
            "['The', 'data', 'we', 'are', 'receiving', 'from', 'MRI', 'scans', 'and', 'DNA', 'testing', 'of', 'these', 'fossils', 'are', '-', 'are', 'staggering', '.']\n",
            "['I', 'mean', ',', 'we', \"'ve\", 'been', 'accepting', 'Leakey', \"'s\", 'dates', 'as', 'a', 'given', ',', 'but', 'if', 'they', \"'re\", 'off', 'by', 'even', 'a', 'hundred', 'thousand', 'years', 'or', 'so', 'then', 'you', 'can', '-', 'you', 'can', 'just', 'throw', 'most', 'of', 'our', 'assumptions', ',', 'you', 'know', ',', 'right', 'in', 'the', 'trash', '.']\n",
            "['So-so', 'what', 'I', 'am', 'saying', 'is', '-', 'is', 'is', 'that', 'is', 'that', 'the', 'repercussions', 'could', 'be', 'huge', '!']\n",
            "['I', 'mean', ',', 'not', 'just', 'in', 'palaeontology', ',', 'but', 'if-if', 'you', 'think', 'about', 'it', ',', 'in', 'evolutionary', 'biology', ',', 'uh', ',', 'genetics', ',', 'geology', ',', 'uh', ',', 'I', 'mean', ',', 'truly', 'the', 'mind', 'boggles', '!']\n",
            "['You', \"'re\", 'right', ',', 'you', \"'re\", 'right', ',', 'it', 'is', '...', 'So', 'you', 'gon', 'na', 'invite', 'us', 'all', 'to', 'the', 'big', 'opening', '?']\n",
            "['Oh', ',', 'we', 'were', 'helping', 'Chandler', 'write', 'his', 'vows', ',', 'but', 'he', 'kicked', 'us', 'out', 'because', 'Joey', 'kept', 'making', 'inappropriate', 'suggestions', '.']\n",
            "['Yeah', ',', 'laugh', 'all', 'you', 'want', 'but', 'in', 'ten', 'minutes', 'we', '?', 're', 'gon', 'na', 'have', 'younger', 'looking', 'skin', '!']\n",
            "['Yeah', ',', 'I', 'guess', 'we', 'don', '?', 't', 'have', 'a', 'choice', '.', 'Help', 'us', '!', 'Please', 'help', 'us', '!', 'We', '?', 're', 'stuck', 'up', 'on', 'the', 'roof', 'and', 'we', 'can', '?', 't', 'get', 'down', '!', '!', '!']\n",
            "['I', 'know', ',', 'I', 'wasn', '?', 't', 'finished', '.', 'But', 'don', '?', 't', 'worry', '!', 'We', '?', 're', 'gon', 'na', 'go', 'down', 'the', 'fire', 'escape', '!', '!']\n",
            "[\"Y'know\", ',', 'here', \"'s\", 'the', 'thing', '.', 'Even', 'if', 'I', 'could', 'get', 'it', 'together', 'enough', 'to-', 'to', 'ask', 'a', 'woman', 'out', ',', '...', 'who', 'am', 'I', 'gon', 'na', 'ask', '?']\n",
            "['Okay', ',', 'and', 'oh', 'I', '?', 'm', 'gon', 'na', 'need', 'a', 'bunch', 'of', 'extra', 'keys', '.', 'Apparently', 'I', 'give', 'them', 'away', 'for', 'no', 'reason', 'at', 'all', '.']\n",
            "['I', 'remember', 'the', 'moonlight', 'coming', 'through', 'the', 'window-', 'and', 'her', 'face', 'had', 'the', 'most', 'incredible', 'glow', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3dtjkc_vEPK"
      },
      "source": [
        "## 2) 형태소 분석"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUk0d9dxp6oH"
      },
      "source": [
        "1) Rachel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlSkeHw2vQqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e68d643-3d60-4ed0-9318-3ff81aceb5fa"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sentences=\"Y'know, ever since I ran out on Barry at the wedding, I have wondered whether I made the right choice.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"I just went to your building and you weren't there and then this guy with a big hammer said you might be here and you are, you are!\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"Ooh! Look! Look! Look! Look, there's Joey's picture! This is so exciting!\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"Ahh, yes, I will have a glass of the Merlot and uh,  he will have a white wine spritzer.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"Hey, time-out, umm, yeah, does the captain know that we?re moving?\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"I think, if it was a little colder in here I could see your nipples through that sweater.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"Hey Mon, what are you doing now? Wanna come see a movie with us?\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"Are you kidding? I'm trained for nothing! I was laughed out of twelve interviews today.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"You would be too if you found John and David boots on sale, fifty percent off!\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"They're my new 'I don't need a job, I don't need my parents, I've got great boots' boots!\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"Well hello! Welcome to Monica's. May I take your coat?\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"No, no, not at the moment, no, I'm not. Are you?\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"Right, yeah, I've heard that about cute doctors.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"Well, actually Gunther sent me. You?re not allowed to have cups out here, it?s a thing.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[(\"Y'know\", 'NN'), (',', ','), ('ever', 'RB'), ('since', 'IN'), ('I', 'PRP'), ('ran', 'VBP'), ('out', 'RP'), ('on', 'IN'), ('Barry', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('wedding', 'NN'), (',', ','), ('I', 'PRP'), ('have', 'VBP'), ('wondered', 'VBN'), ('whether', 'IN'), ('I', 'PRP'), ('made', 'VBD'), ('the', 'DT'), ('right', 'JJ'), ('choice', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('just', 'RB'), ('went', 'VBD'), ('to', 'TO'), ('your', 'PRP$'), ('building', 'NN'), ('and', 'CC'), ('you', 'PRP'), ('were', 'VBD'), (\"n't\", 'RB'), ('there', 'RB'), ('and', 'CC'), ('then', 'RB'), ('this', 'DT'), ('guy', 'NN'), ('with', 'IN'), ('a', 'DT'), ('big', 'JJ'), ('hammer', 'NN'), ('said', 'VBD'), ('you', 'PRP'), ('might', 'MD'), ('be', 'VB'), ('here', 'RB'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), (',', ','), ('you', 'PRP'), ('are', 'VBP'), ('!', '.')]\n",
            "[('Ooh', 'NN'), ('!', '.'), ('Look', 'NN'), ('!', '.'), ('Look', 'NN'), ('!', '.'), ('Look', 'NN'), ('!', '.'), ('Look', 'NNP'), (',', ','), ('there', 'EX'), (\"'s\", 'VBZ'), ('Joey', 'NNP'), (\"'s\", 'POS'), ('picture', 'NN'), ('!', '.'), ('This', 'DT'), ('is', 'VBZ'), ('so', 'RB'), ('exciting', 'JJ'), ('!', '.')]\n",
            "[('Ahh', 'NNP'), (',', ','), ('yes', 'UH'), (',', ','), ('I', 'PRP'), ('will', 'MD'), ('have', 'VB'), ('a', 'DT'), ('glass', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Merlot', 'NNP'), ('and', 'CC'), ('uh', 'UH'), (',', ','), ('he', 'PRP'), ('will', 'MD'), ('have', 'VB'), ('a', 'DT'), ('white', 'JJ'), ('wine', 'NN'), ('spritzer', 'NN'), ('.', '.')]\n",
            "[('Hey', 'NNP'), (',', ','), ('time-out', 'NN'), (',', ','), ('umm', 'JJ'), (',', ','), ('yeah', 'UH'), (',', ','), ('does', 'VBZ'), ('the', 'DT'), ('captain', 'NN'), ('know', 'VBP'), ('that', 'IN'), ('we', 'PRP'), ('?', '.'), ('re', 'VB'), ('moving', 'VBG'), ('?', '.')]\n",
            "[('I', 'PRP'), ('think', 'VBP'), (',', ','), ('if', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('a', 'DT'), ('little', 'JJ'), ('colder', 'NN'), ('in', 'IN'), ('here', 'RB'), ('I', 'PRP'), ('could', 'MD'), ('see', 'VB'), ('your', 'PRP$'), ('nipples', 'NNS'), ('through', 'IN'), ('that', 'DT'), ('sweater', 'NN'), ('.', '.')]\n",
            "[('Hey', 'NNP'), ('Mon', 'NNP'), (',', ','), ('what', 'WP'), ('are', 'VBP'), ('you', 'PRP'), ('doing', 'VBG'), ('now', 'RB'), ('?', '.'), ('Wan', 'NNP'), ('na', 'CC'), ('come', 'VBN'), ('see', 'VBP'), ('a', 'DT'), ('movie', 'NN'), ('with', 'IN'), ('us', 'PRP'), ('?', '.')]\n",
            "[('Are', 'NNP'), ('you', 'PRP'), ('kidding', 'VBG'), ('?', '.'), ('I', 'PRP'), (\"'m\", 'VBP'), ('trained', 'JJ'), ('for', 'IN'), ('nothing', 'NN'), ('!', '.'), ('I', 'PRP'), ('was', 'VBD'), ('laughed', 'VBN'), ('out', 'IN'), ('of', 'IN'), ('twelve', 'NN'), ('interviews', 'NNS'), ('today', 'NN'), ('.', '.')]\n",
            "[('You', 'PRP'), ('would', 'MD'), ('be', 'VB'), ('too', 'RB'), ('if', 'IN'), ('you', 'PRP'), ('found', 'VBP'), ('John', 'NNP'), ('and', 'CC'), ('David', 'NNP'), ('boots', 'NNS'), ('on', 'IN'), ('sale', 'NN'), (',', ','), ('fifty', 'JJ'), ('percent', 'NN'), ('off', 'IN'), ('!', '.')]\n",
            "[('They', 'PRP'), (\"'re\", 'VBP'), ('my', 'PRP$'), ('new', 'JJ'), (\"'I\", 'NNS'), ('do', 'VBP'), (\"n't\", 'RB'), ('need', 'VB'), ('a', 'DT'), ('job', 'NN'), (',', ','), ('I', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('need', 'VB'), ('my', 'PRP$'), ('parents', 'NNS'), (',', ','), ('I', 'PRP'), (\"'ve\", 'VBP'), ('got', 'VBN'), ('great', 'JJ'), ('boots', 'NNS'), (\"'\", 'POS'), ('boots', 'NNS'), ('!', '.')]\n",
            "[('Well', 'RB'), ('hello', 'RB'), ('!', '.'), ('Welcome', 'NNP'), ('to', 'TO'), ('Monica', 'NNP'), (\"'s\", 'POS'), ('.', '.'), ('May', 'NNP'), ('I', 'PRP'), ('take', 'VBP'), ('your', 'PRP$'), ('coat', 'NN'), ('?', '.')]\n",
            "[('No', 'DT'), (',', ','), ('no', 'DT'), (',', ','), ('not', 'RB'), ('at', 'IN'), ('the', 'DT'), ('moment', 'NN'), (',', ','), ('no', 'DT'), (',', ','), ('I', 'PRP'), (\"'m\", 'VBP'), ('not', 'RB'), ('.', '.'), ('Are', 'VB'), ('you', 'PRP'), ('?', '.')]\n",
            "[('Right', 'RB'), (',', ','), ('yeah', 'UH'), (',', ','), ('I', 'PRP'), (\"'ve\", 'VBP'), ('heard', 'VBN'), ('that', 'IN'), ('about', 'IN'), ('cute', 'NN'), ('doctors', 'NNS'), ('.', '.')]\n",
            "[('Well', 'RB'), (',', ','), ('actually', 'RB'), ('Gunther', 'NNP'), ('sent', 'VBD'), ('me', 'PRP'), ('.', '.'), ('You', 'PRP'), ('?', '.'), ('re', 'VB'), ('not', 'RB'), ('allowed', 'VBN'), ('to', 'TO'), ('have', 'VB'), ('cups', 'NNS'), ('out', 'RP'), ('here', 'RB'), (',', ','), ('it', 'PRP'), ('?', '.'), ('s', 'VBZ'), ('a', 'DT'), ('thing', 'NN'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aH_tRvFw7df"
      },
      "source": [
        "2) Monica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMmLL92Yw7ln",
        "outputId": "72a13077-e909-48d0-bd19-dedd6910b250"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "\r\n",
        "sentences=\"Come on.  Hello?  I?m sorry you have the wrong number.   Okay, I?ll call you later dad. I love you.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Hello? No rejection? I got shot down at fat camp! Boy, kids are mean when they?re hungry.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"I mean, what are you gonna do, never going to talk to her again?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"I mean I know it?s weird, it?s awkward, but you gotta at least try.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Okay, everybody, this is Rachel, another Lincoln High survivor.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"This is everybody, this is Chandler, and Phoebe, and Joey, and- you remember my brother Ross?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Probably some y'know, European good-bye thing he picked up in London.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Well, the end table is wrong, The couch looks bizarre and don't even get me started on the refrigerator magnets.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"She?s a hooker! She?s a hooker! She?s a?  Hi! Uh, we spoke on the phone.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"I know this is going to sound unbelievably selfish, but, were you planning on bringing up the whole baby/lesbian thing?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Hey guys! Do you wanna look at the song list for the wedding?  Guys?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"I?m sorry. I?m sorry. I-I should probably leave you girls alone.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Hey Frannie, welcome back! How was Florida?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"You mean you know Paul like I know Paul?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[('Come', 'VBN'), ('on', 'IN'), ('.', '.'), ('Hello', 'NNP'), ('?', '.'), ('I', 'PRP'), ('?', '.'), ('m', 'NN'), ('sorry', 'NN'), ('you', 'PRP'), ('have', 'VBP'), ('the', 'DT'), ('wrong', 'JJ'), ('number', 'NN'), ('.', '.'), ('Okay', 'NNP'), (',', ','), ('I', 'PRP'), ('?', '.'), ('ll', \"''\"), ('call', 'NN'), ('you', 'PRP'), ('later', 'RB'), ('dad', 'VBP'), ('.', '.'), ('I', 'PRP'), ('love', 'VBP'), ('you', 'PRP'), ('.', '.')]\n",
            "[('Hello', 'NNP'), ('?', '.'), ('No', 'DT'), ('rejection', 'NN'), ('?', '.'), ('I', 'PRP'), ('got', 'VBD'), ('shot', 'RB'), ('down', 'RB'), ('at', 'IN'), ('fat', 'JJ'), ('camp', 'NN'), ('!', '.'), ('Boy', 'NNP'), (',', ','), ('kids', 'NNS'), ('are', 'VBP'), ('mean', 'JJ'), ('when', 'WRB'), ('they', 'PRP'), ('?', '.'), ('re', 'NN'), ('hungry', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('mean', 'VBP'), (',', ','), ('what', 'WP'), ('are', 'VBP'), ('you', 'PRP'), ('gon', 'VB'), ('na', 'TO'), ('do', 'VB'), (',', ','), ('never', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('talk', 'VB'), ('to', 'TO'), ('her', 'PRP$'), ('again', 'RB'), ('?', '.')]\n",
            "[('I', 'PRP'), ('mean', 'VBP'), ('I', 'PRP'), ('know', 'VBP'), ('it', 'PRP'), ('?', '.'), ('s', 'JJ'), ('weird', 'NN'), (',', ','), ('it', 'PRP'), ('?', '.'), ('s', 'JJ'), ('awkward', 'NN'), (',', ','), ('but', 'CC'), ('you', 'PRP'), ('got', 'VBD'), ('ta', 'NNS'), ('at', 'IN'), ('least', 'JJS'), ('try', 'NN'), ('.', '.')]\n",
            "[('Okay', 'NNP'), (',', ','), ('everybody', 'NN'), (',', ','), ('this', 'DT'), ('is', 'VBZ'), ('Rachel', 'NNP'), (',', ','), ('another', 'DT'), ('Lincoln', 'NNP'), ('High', 'NNP'), ('survivor', 'NN'), ('.', '.')]\n",
            "[('This', 'DT'), ('is', 'VBZ'), ('everybody', 'NN'), (',', ','), ('this', 'DT'), ('is', 'VBZ'), ('Chandler', 'NNP'), (',', ','), ('and', 'CC'), ('Phoebe', 'NNP'), (',', ','), ('and', 'CC'), ('Joey', 'NNP'), (',', ','), ('and-', 'NN'), ('you', 'PRP'), ('remember', 'VBP'), ('my', 'PRP$'), ('brother', 'NN'), ('Ross', 'NNP'), ('?', '.')]\n",
            "[('Probably', 'RB'), ('some', 'DT'), (\"y'know\", 'NN'), (',', ','), ('European', 'JJ'), ('good-bye', 'JJ'), ('thing', 'NN'), ('he', 'PRP'), ('picked', 'VBD'), ('up', 'RP'), ('in', 'IN'), ('London', 'NNP'), ('.', '.')]\n",
            "[('Well', 'RB'), (',', ','), ('the', 'DT'), ('end', 'NN'), ('table', 'NN'), ('is', 'VBZ'), ('wrong', 'JJ'), (',', ','), ('The', 'DT'), ('couch', 'JJ'), ('looks', 'NNS'), ('bizarre', 'JJ'), ('and', 'CC'), ('do', 'VBP'), (\"n't\", 'RB'), ('even', 'RB'), ('get', 'VB'), ('me', 'PRP'), ('started', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('refrigerator', 'NN'), ('magnets', 'NNS'), ('.', '.')]\n",
            "[('She', 'PRP'), ('?', '.'), ('s', 'VB'), ('a', 'DT'), ('hooker', 'NN'), ('!', '.'), ('She', 'PRP'), ('?', '.'), ('s', 'VB'), ('a', 'DT'), ('hooker', 'NN'), ('!', '.'), ('She', 'PRP'), ('?', '.'), ('s', 'VB'), ('a', 'DT'), ('?', '.'), ('Hi', 'NN'), ('!', '.'), ('Uh', 'NNP'), (',', ','), ('we', 'PRP'), ('spoke', 'VBD'), ('on', 'IN'), ('the', 'DT'), ('phone', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('know', 'VBP'), ('this', 'DT'), ('is', 'VBZ'), ('going', 'VBG'), ('to', 'TO'), ('sound', 'VB'), ('unbelievably', 'RB'), ('selfish', 'JJ'), (',', ','), ('but', 'CC'), (',', ','), ('were', 'VBD'), ('you', 'PRP'), ('planning', 'VBG'), ('on', 'IN'), ('bringing', 'VBG'), ('up', 'RP'), ('the', 'DT'), ('whole', 'JJ'), ('baby/lesbian', 'JJ'), ('thing', 'NN'), ('?', '.')]\n",
            "[('Hey', 'NNP'), ('guys', 'NNS'), ('!', '.'), ('Do', 'VBP'), ('you', 'PRP'), ('wan', 'VB'), ('na', 'JJ'), ('look', 'NN'), ('at', 'IN'), ('the', 'DT'), ('song', 'JJ'), ('list', 'NN'), ('for', 'IN'), ('the', 'DT'), ('wedding', 'NN'), ('?', '.'), ('Guys', 'NNP'), ('?', '.')]\n",
            "[('I', 'PRP'), ('?', '.'), ('m', 'NN'), ('sorry', 'NN'), ('.', '.'), ('I', 'PRP'), ('?', '.'), ('m', 'NN'), ('sorry', 'NN'), ('.', '.'), ('I-I', 'NNP'), ('should', 'MD'), ('probably', 'RB'), ('leave', 'VB'), ('you', 'PRP'), ('girls', 'VB'), ('alone', 'RB'), ('.', '.')]\n",
            "[('Hey', 'NNP'), ('Frannie', 'NNP'), (',', ','), ('welcome', 'VB'), ('back', 'RB'), ('!', '.'), ('How', 'NN'), ('was', 'VBD'), ('Florida', 'NNP'), ('?', '.')]\n",
            "[('You', 'PRP'), ('mean', 'VBP'), ('you', 'PRP'), ('know', 'VBP'), ('Paul', 'NNP'), ('like', 'IN'), ('I', 'PRP'), ('know', 'VBP'), ('Paul', 'NNP'), ('?', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgAKRWyaxr8E"
      },
      "source": [
        "3) Chandler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOG6PoUnxjwT",
        "outputId": "2e8ae1ba-ea7f-4384-db9b-d4db7efbef5e"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "\r\n",
        "sentences=\"I don?t want him to tell this story for years.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"All right! Go left! Go left! Go right!! Go right!!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"So what do you say, maybe sometime I hold your gun?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"You can always spot someone who's never seen one of his plays before. Notice, no fear, no sense of impending doom...\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"I got caught up and work, but I'm quitting tomorrow.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Oh, why not. Was I doing anything particularly... saucy?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"No, I don't see anything different other than the fact that the room got so much brighter when you came into it.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Yeah, I miss that too. I tell you what; from now on we?ll make time to hang out with each other.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"I really shouldn?t have said that you were embarrassing me, I mean that really wasn?t cool.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"And if it makes you feel any better, I?ve had a really lousy day.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Oh, y'know, what did you mean when you said pivot?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Yes, the moon, the glow, the magical feeling, you did this part- Could I get some painkillers over here, please?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[('I', 'PRP'), ('don', 'VBP'), ('?', '.'), ('t', 'NN'), ('want', 'VBP'), ('him', 'PRP'), ('to', 'TO'), ('tell', 'VB'), ('this', 'DT'), ('story', 'NN'), ('for', 'IN'), ('years', 'NNS'), ('.', '.')]\n",
            "[('All', 'DT'), ('right', 'NN'), ('!', '.'), ('Go', 'VB'), ('left', 'JJ'), ('!', '.'), ('Go', 'VB'), ('left', 'JJ'), ('!', '.'), ('Go', 'VB'), ('right', 'JJ'), ('!', '.'), ('!', '.'), ('Go', 'VB'), ('right', 'JJ'), ('!', '.'), ('!', '.')]\n",
            "[('So', 'RB'), ('what', 'WP'), ('do', 'VBP'), ('you', 'PRP'), ('say', 'VB'), (',', ','), ('maybe', 'RB'), ('sometime', 'RB'), ('I', 'PRP'), ('hold', 'VBP'), ('your', 'PRP$'), ('gun', 'NN'), ('?', '.')]\n",
            "[('You', 'PRP'), ('can', 'MD'), ('always', 'RB'), ('spot', 'VB'), ('someone', 'NN'), ('who', 'WP'), (\"'s\", 'VBZ'), ('never', 'RB'), ('seen', 'VBN'), ('one', 'CD'), ('of', 'IN'), ('his', 'PRP$'), ('plays', 'NNS'), ('before', 'IN'), ('.', '.'), ('Notice', 'NNP'), (',', ','), ('no', 'DT'), ('fear', 'NN'), (',', ','), ('no', 'DT'), ('sense', 'NN'), ('of', 'IN'), ('impending', 'VBG'), ('doom', 'NN'), ('...', ':')]\n",
            "[('I', 'PRP'), ('got', 'VBD'), ('caught', 'VBN'), ('up', 'RP'), ('and', 'CC'), ('work', 'NN'), (',', ','), ('but', 'CC'), ('I', 'PRP'), (\"'m\", 'VBP'), ('quitting', 'VBG'), ('tomorrow', 'NN'), ('.', '.')]\n",
            "[('Oh', 'UH'), (',', ','), ('why', 'WRB'), ('not', 'RB'), ('.', '.'), ('Was', 'NNP'), ('I', 'PRP'), ('doing', 'VBG'), ('anything', 'NN'), ('particularly', 'RB'), ('...', ':'), ('saucy', 'NN'), ('?', '.')]\n",
            "[('No', 'DT'), (',', ','), ('I', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('see', 'VB'), ('anything', 'NN'), ('different', 'JJ'), ('other', 'JJ'), ('than', 'IN'), ('the', 'DT'), ('fact', 'NN'), ('that', 'IN'), ('the', 'DT'), ('room', 'NN'), ('got', 'VBD'), ('so', 'RB'), ('much', 'JJ'), ('brighter', 'NN'), ('when', 'WRB'), ('you', 'PRP'), ('came', 'VBD'), ('into', 'IN'), ('it', 'PRP'), ('.', '.')]\n",
            "[('Yeah', 'UH'), (',', ','), ('I', 'PRP'), ('miss', 'VBP'), ('that', 'DT'), ('too', 'RB'), ('.', '.'), ('I', 'PRP'), ('tell', 'VBP'), ('you', 'PRP'), ('what', 'WP'), (';', ':'), ('from', 'IN'), ('now', 'RB'), ('on', 'IN'), ('we', 'PRP'), ('?', '.'), ('ll', 'VB'), ('make', 'JJ'), ('time', 'NN'), ('to', 'TO'), ('hang', 'VB'), ('out', 'RP'), ('with', 'IN'), ('each', 'DT'), ('other', 'JJ'), ('.', '.')]\n",
            "[('I', 'PRP'), ('really', 'RB'), ('shouldn', 'VB'), ('?', '.'), ('t', 'NNS'), ('have', 'VBP'), ('said', 'VBD'), ('that', 'IN'), ('you', 'PRP'), ('were', 'VBD'), ('embarrassing', 'VBG'), ('me', 'PRP'), (',', ','), ('I', 'PRP'), ('mean', 'VBP'), ('that', 'IN'), ('really', 'RB'), ('wasn', 'VB'), ('?', '.'), ('t', 'JJ'), ('cool', 'NN'), ('.', '.')]\n",
            "[('And', 'CC'), ('if', 'IN'), ('it', 'PRP'), ('makes', 'VBZ'), ('you', 'PRP'), ('feel', 'VBP'), ('any', 'DT'), ('better', 'JJR'), (',', ','), ('I', 'PRP'), ('?', '.'), ('ve', 'NN'), ('had', 'VBD'), ('a', 'DT'), ('really', 'RB'), ('lousy', 'JJ'), ('day', 'NN'), ('.', '.')]\n",
            "[('Oh', 'UH'), (',', ','), (\"y'know\", 'UH'), (',', ','), ('what', 'WP'), ('did', 'VBD'), ('you', 'PRP'), ('mean', 'VB'), ('when', 'WRB'), ('you', 'PRP'), ('said', 'VBD'), ('pivot', 'NN'), ('?', '.')]\n",
            "[('Yes', 'UH'), (',', ','), ('the', 'DT'), ('moon', 'NN'), (',', ','), ('the', 'DT'), ('glow', 'NN'), (',', ','), ('the', 'DT'), ('magical', 'JJ'), ('feeling', 'NN'), (',', ','), ('you', 'PRP'), ('did', 'VBD'), ('this', 'DT'), ('part-', 'NN'), ('Could', 'NNP'), ('I', 'PRP'), ('get', 'VBP'), ('some', 'DT'), ('painkillers', 'NNS'), ('over', 'IN'), ('here', 'RB'), (',', ','), ('please', 'VB'), ('?', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFNyDqhJyJyp"
      },
      "source": [
        "4) Joey"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcdXDOchyKBn",
        "outputId": "3021cd6c-b61f-4e8b-921d-dcae5ae84b61"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "\r\n",
        "sentences=\"Then you gotta come clean with Ma! This is not right!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Listen, uh, what do you say I buy you that cup of coffee now?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Whoa-whoa! No-no-no-no-no, nothing is going up! Okay? Up, up is not an option?what's a urethra?  Are you crazy?!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Jo?s there, but I don?t think there?s anything she could do.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Oh thanks. Thanks. It was great meetin? ya. And listen if any of my friends gets married, or have a birthday, or a Tuesday\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Well, Chandler's old roomate was Jewish, and these are the only candles we have, so... Happy Chanukah, everyone.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Vell, Eva, ve've done some excellent vork here, and I vould have to say, your pwoblem is qviiite clear.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Oh sure?And hey, don?t get me wrong, I am so happy for you guys. I just?I miss?hanging out?just-just us, y?know?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"He seemed like a stand up guy. Oh, and he?s not into anything weird sexually.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"All right well, y?know?I guess we know what we have to do to get down.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Ross. I was thinking we could just go down the fire escape.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Noo!! I?ve had the best day ever! Dude, check this out!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Great story!? But, I uh, I gotta go, I got a date with Andrea--Angela--Andrea...? Oh man,\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[('Then', 'RB'), ('you', 'PRP'), ('got', 'VBD'), ('ta', 'JJ'), ('come', 'JJ'), ('clean', 'NN'), ('with', 'IN'), ('Ma', 'NNP'), ('!', '.'), ('This', 'DT'), ('is', 'VBZ'), ('not', 'RB'), ('right', 'JJ'), ('!', '.')]\n",
            "[('Listen', 'NNP'), (',', ','), ('uh', 'UH'), (',', ','), ('what', 'WP'), ('do', 'VBP'), ('you', 'PRP'), ('say', 'VBP'), ('I', 'PRP'), ('buy', 'VBP'), ('you', 'PRP'), ('that', 'IN'), ('cup', 'NN'), ('of', 'IN'), ('coffee', 'NN'), ('now', 'RB'), ('?', '.')]\n",
            "[('Whoa-whoa', 'JJ'), ('!', '.'), ('No-no-no-no-no', 'NN'), (',', ','), ('nothing', 'NN'), ('is', 'VBZ'), ('going', 'VBG'), ('up', 'RP'), ('!', '.'), ('Okay', 'NN'), ('?', '.'), ('Up', 'UH'), (',', ','), ('up', 'RB'), ('is', 'VBZ'), ('not', 'RB'), ('an', 'DT'), ('option', 'NN'), ('?', '.'), ('what', 'WP'), (\"'s\", 'VBZ'), ('a', 'DT'), ('urethra', 'NN'), ('?', '.'), ('Are', 'NNP'), ('you', 'PRP'), ('crazy', 'JJ'), ('?', '.'), ('!', '.')]\n",
            "[('Jo', 'NNP'), ('?', '.'), ('s', 'NN'), ('there', 'RB'), (',', ','), ('but', 'CC'), ('I', 'PRP'), ('don', 'VBP'), ('?', '.'), ('t', 'JJ'), ('think', 'NN'), ('there', 'EX'), ('?', '.'), ('s', 'NN'), ('anything', 'NN'), ('she', 'PRP'), ('could', 'MD'), ('do', 'VB'), ('.', '.')]\n",
            "[('Oh', 'UH'), ('thanks', 'NNS'), ('.', '.'), ('Thanks', 'NNS'), ('.', '.'), ('It', 'PRP'), ('was', 'VBD'), ('great', 'JJ'), ('meetin', 'NN'), ('?', '.'), ('ya', 'NN'), ('.', '.'), ('And', 'CC'), ('listen', 'VB'), ('if', 'IN'), ('any', 'DT'), ('of', 'IN'), ('my', 'PRP$'), ('friends', 'NNS'), ('gets', 'VBZ'), ('married', 'JJ'), (',', ','), ('or', 'CC'), ('have', 'VBP'), ('a', 'DT'), ('birthday', 'NN'), (',', ','), ('or', 'CC'), ('a', 'DT'), ('Tuesday', 'NNP')]\n",
            "[('Well', 'RB'), (',', ','), ('Chandler', 'NNP'), (\"'s\", 'POS'), ('old', 'JJ'), ('roomate', 'NN'), ('was', 'VBD'), ('Jewish', 'JJ'), (',', ','), ('and', 'CC'), ('these', 'DT'), ('are', 'VBP'), ('the', 'DT'), ('only', 'JJ'), ('candles', 'VBZ'), ('we', 'PRP'), ('have', 'VBP'), (',', ','), ('so', 'RB'), ('...', ':'), ('Happy', 'JJ'), ('Chanukah', 'NNP'), (',', ','), ('everyone', 'NN'), ('.', '.')]\n",
            "[('Vell', 'NNP'), (',', ','), ('Eva', 'NNP'), (',', ','), ('ve', 'NN'), (\"'ve\", 'VBP'), ('done', 'VBN'), ('some', 'DT'), ('excellent', 'JJ'), ('vork', 'NN'), ('here', 'RB'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('vould', 'VBP'), ('have', 'VB'), ('to', 'TO'), ('say', 'VB'), (',', ','), ('your', 'PRP$'), ('pwoblem', 'NN'), ('is', 'VBZ'), ('qviiite', 'JJ'), ('clear', 'JJ'), ('.', '.')]\n",
            "[('Oh', 'UH'), ('sure', 'JJ'), ('?', '.'), ('And', 'CC'), ('hey', 'NN'), (',', ','), ('don', 'VB'), ('?', '.'), ('t', 'NN'), ('get', 'VB'), ('me', 'PRP'), ('wrong', 'JJ'), (',', ','), ('I', 'PRP'), ('am', 'VBP'), ('so', 'RB'), ('happy', 'JJ'), ('for', 'IN'), ('you', 'PRP'), ('guys', 'VBP'), ('.', '.'), ('I', 'PRP'), ('just', 'RB'), ('?', '.'), ('I', 'PRP'), ('miss', 'VBP'), ('?', '.'), ('hanging', 'VBG'), ('out', 'RP'), ('?', '.'), ('just-just', 'JJ'), ('us', 'PRP'), (',', ','), ('y', 'VB'), ('?', '.'), ('know', 'VB'), ('?', '.')]\n",
            "[('He', 'PRP'), ('seemed', 'VBD'), ('like', 'IN'), ('a', 'DT'), ('stand', 'NN'), ('up', 'RP'), ('guy', 'NN'), ('.', '.'), ('Oh', 'UH'), (',', ','), ('and', 'CC'), ('he', 'PRP'), ('?', '.'), ('s', 'VBZ'), ('not', 'RB'), ('into', 'IN'), ('anything', 'NN'), ('weird', 'JJ'), ('sexually', 'RB'), ('.', '.')]\n",
            "[('All', 'DT'), ('right', 'RB'), ('well', 'RB'), (',', ','), ('y', 'PRP'), ('?', '.'), ('know', 'VB'), ('?', '.'), ('I', 'PRP'), ('guess', 'VBP'), ('we', 'PRP'), ('know', 'VBP'), ('what', 'WP'), ('we', 'PRP'), ('have', 'VBP'), ('to', 'TO'), ('do', 'VB'), ('to', 'TO'), ('get', 'VB'), ('down', 'RP'), ('.', '.')]\n",
            "[('Ross', 'NNP'), ('.', '.'), ('I', 'PRP'), ('was', 'VBD'), ('thinking', 'VBG'), ('we', 'PRP'), ('could', 'MD'), ('just', 'RB'), ('go', 'VB'), ('down', 'RP'), ('the', 'DT'), ('fire', 'NN'), ('escape', 'NN'), ('.', '.')]\n",
            "[('Noo', 'NN'), ('!', '.'), ('!', '.'), ('I', 'PRP'), ('?', '.'), ('ve', 'NN'), ('had', 'VBD'), ('the', 'DT'), ('best', 'JJS'), ('day', 'NN'), ('ever', 'RB'), ('!', '.'), ('Dude', 'NNP'), (',', ','), ('check', 'VB'), ('this', 'DT'), ('out', 'RP'), ('!', '.')]\n",
            "[('Great', 'NNP'), ('story', 'NN'), ('!', '.'), ('?', '.'), ('But', 'CC'), (',', ','), ('I', 'PRP'), ('uh', 'VBP'), (',', ','), ('I', 'PRP'), ('got', 'VBD'), ('ta', 'RB'), ('go', 'VB'), (',', ','), ('I', 'PRP'), ('got', 'VBD'), ('a', 'DT'), ('date', 'NN'), ('with', 'IN'), ('Andrea', 'NNP'), ('--', ':'), ('Angela', 'NNP'), ('--', ':'), ('Andrea', 'VBP'), ('...', ':'), ('?', '.'), ('Oh', 'UH'), ('man', 'NN'), (',', ',')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Redd14wCye3V"
      },
      "source": [
        "5) Ross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "towZH3ZwyfEo",
        "outputId": "df0f2d93-9b97-4e42-82fc-af3a1d7276ad"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "\r\n",
        "sentences=\"Oh, but he will. He still tells the story how Monica tried to escape from fat camp.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Okay, Andre should be there in like 45 minutes. All rightie, bye bye.  Just easier that way.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"The data we are receiving from MRI scans and DNA testing of these fossils are - are staggering.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"I mean, we've been accepting Leakey's dates as a given, but if they're off by even a hundred thousand years or so then you can - you can just throw most of our assumptions, you know, right in the trash.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"So-so what I am saying is - is is that  is that the repercussions could be huge!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"I mean, not just in palaeontology, but if-if you think about it, in evolutionary biology, uh, genetics, geology, uh, I mean, truly the mind boggles!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"You're right, you're right, it is...So you gonna invite us all to the big opening?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Oh, we were helping Chandler write his vows, but he kicked us out because Joey kept making inappropriate suggestions.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Yeah, laugh all you want but in ten minutes we?re gonna have younger looking skin!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Yeah, I guess we don?t have a choice.  Help us! Please help us! We?re stuck up on the roof and we can?t get down!!!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"I know, I wasn?t finished.   But don?t worry! We?re gonna go down the fire escape!!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Y'know, here's the thing. Even if I could get it together enough to- to ask a woman out,... who am I gonna ask?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Okay, and oh I?m gonna need a bunch of extra keys. Apparently I give them away for no reason at all.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"I remember the moonlight coming through the window- and her face had the most incredible glow.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[('Oh', 'UH'), (',', ','), ('but', 'CC'), ('he', 'PRP'), ('will', 'MD'), ('.', '.'), ('He', 'PRP'), ('still', 'RB'), ('tells', 'VBZ'), ('the', 'DT'), ('story', 'NN'), ('how', 'WRB'), ('Monica', 'NNP'), ('tried', 'VBD'), ('to', 'TO'), ('escape', 'VB'), ('from', 'IN'), ('fat', 'JJ'), ('camp', 'NN'), ('.', '.')]\n",
            "[('Okay', 'NNP'), (',', ','), ('Andre', 'NNP'), ('should', 'MD'), ('be', 'VB'), ('there', 'RB'), ('in', 'IN'), ('like', 'IN'), ('45', 'CD'), ('minutes', 'NNS'), ('.', '.'), ('All', 'DT'), ('rightie', 'NN'), (',', ','), ('bye', 'NN'), ('bye', 'NN'), ('.', '.'), ('Just', 'RB'), ('easier', 'JJR'), ('that', 'DT'), ('way', 'NN'), ('.', '.')]\n",
            "[('The', 'DT'), ('data', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('receiving', 'VBG'), ('from', 'IN'), ('MRI', 'NNP'), ('scans', 'NNS'), ('and', 'CC'), ('DNA', 'NNP'), ('testing', 'NN'), ('of', 'IN'), ('these', 'DT'), ('fossils', 'NNS'), ('are', 'VBP'), ('-', ':'), ('are', 'VBP'), ('staggering', 'VBG'), ('.', '.')]\n",
            "[('I', 'PRP'), ('mean', 'VBP'), (',', ','), ('we', 'PRP'), (\"'ve\", 'VBP'), ('been', 'VBN'), ('accepting', 'VBG'), ('Leakey', 'NNP'), (\"'s\", 'POS'), ('dates', 'NNS'), ('as', 'IN'), ('a', 'DT'), ('given', 'VBN'), (',', ','), ('but', 'CC'), ('if', 'IN'), ('they', 'PRP'), (\"'re\", 'VBP'), ('off', 'IN'), ('by', 'IN'), ('even', 'RB'), ('a', 'DT'), ('hundred', 'VBN'), ('thousand', 'CD'), ('years', 'NNS'), ('or', 'CC'), ('so', 'RB'), ('then', 'RB'), ('you', 'PRP'), ('can', 'MD'), ('-', ':'), ('you', 'PRP'), ('can', 'MD'), ('just', 'RB'), ('throw', 'VB'), ('most', 'JJS'), ('of', 'IN'), ('our', 'PRP$'), ('assumptions', 'NNS'), (',', ','), ('you', 'PRP'), ('know', 'VBP'), (',', ','), ('right', 'RB'), ('in', 'IN'), ('the', 'DT'), ('trash', 'NN'), ('.', '.')]\n",
            "[('So-so', 'JJ'), ('what', 'WP'), ('I', 'PRP'), ('am', 'VBP'), ('saying', 'VBG'), ('is', 'VBZ'), ('-', ':'), ('is', 'VBZ'), ('is', 'VBZ'), ('that', 'DT'), ('is', 'VBZ'), ('that', 'IN'), ('the', 'DT'), ('repercussions', 'NNS'), ('could', 'MD'), ('be', 'VB'), ('huge', 'JJ'), ('!', '.')]\n",
            "[('I', 'PRP'), ('mean', 'VBP'), (',', ','), ('not', 'RB'), ('just', 'RB'), ('in', 'IN'), ('palaeontology', 'NN'), (',', ','), ('but', 'CC'), ('if-if', 'JJ'), ('you', 'PRP'), ('think', 'VBP'), ('about', 'IN'), ('it', 'PRP'), (',', ','), ('in', 'IN'), ('evolutionary', 'JJ'), ('biology', 'NN'), (',', ','), ('uh', 'UH'), (',', ','), ('genetics', 'NNS'), (',', ','), ('geology', 'NN'), (',', ','), ('uh', 'UH'), (',', ','), ('I', 'PRP'), ('mean', 'VBP'), (',', ','), ('truly', 'RB'), ('the', 'DT'), ('mind', 'NN'), ('boggles', 'VBZ'), ('!', '.')]\n",
            "[('You', 'PRP'), (\"'re\", 'VBP'), ('right', 'JJ'), (',', ','), ('you', 'PRP'), (\"'re\", 'VBP'), ('right', 'JJ'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('...', ':'), ('So', 'RB'), ('you', 'PRP'), ('gon', 'VBP'), ('na', 'TO'), ('invite', 'VB'), ('us', 'PRP'), ('all', 'DT'), ('to', 'TO'), ('the', 'DT'), ('big', 'JJ'), ('opening', 'NN'), ('?', '.')]\n",
            "[('Oh', 'UH'), (',', ','), ('we', 'PRP'), ('were', 'VBD'), ('helping', 'VBG'), ('Chandler', 'NNP'), ('write', 'VB'), ('his', 'PRP$'), ('vows', 'NNS'), (',', ','), ('but', 'CC'), ('he', 'PRP'), ('kicked', 'VBD'), ('us', 'PRP'), ('out', 'RP'), ('because', 'IN'), ('Joey', 'NNP'), ('kept', 'VBD'), ('making', 'VBG'), ('inappropriate', 'JJ'), ('suggestions', 'NNS'), ('.', '.')]\n",
            "[('Yeah', 'UH'), (',', ','), ('laugh', 'IN'), ('all', 'DT'), ('you', 'PRP'), ('want', 'VBP'), ('but', 'CC'), ('in', 'IN'), ('ten', 'JJ'), ('minutes', 'NNS'), ('we', 'PRP'), ('?', '.'), ('re', 'VB'), ('gon', 'NN'), ('na', 'NNS'), ('have', 'VBP'), ('younger', 'JJR'), ('looking', 'VBG'), ('skin', 'NN'), ('!', '.')]\n",
            "[('Yeah', 'UH'), (',', ','), ('I', 'PRP'), ('guess', 'VBP'), ('we', 'PRP'), ('don', 'VB'), ('?', '.'), ('t', 'NNS'), ('have', 'VBP'), ('a', 'DT'), ('choice', 'NN'), ('.', '.'), ('Help', 'VB'), ('us', 'PRP'), ('!', '.'), ('Please', 'VB'), ('help', 'VB'), ('us', 'PRP'), ('!', '.'), ('We', 'PRP'), ('?', '.'), ('re', 'VB'), ('stuck', 'VBN'), ('up', 'RP'), ('on', 'IN'), ('the', 'DT'), ('roof', 'NN'), ('and', 'CC'), ('we', 'PRP'), ('can', 'MD'), ('?', '.'), ('t', 'VB'), ('get', 'VB'), ('down', 'RP'), ('!', '.'), ('!', '.'), ('!', '.')]\n",
            "[('I', 'PRP'), ('know', 'VBP'), (',', ','), ('I', 'PRP'), ('wasn', 'VBP'), ('?', '.'), ('t', 'NN'), ('finished', 'VBN'), ('.', '.'), ('But', 'CC'), ('don', 'VB'), ('?', '.'), ('t', 'NN'), ('worry', 'NN'), ('!', '.'), ('We', 'PRP'), ('?', '.'), ('re', 'VB'), ('gon', 'NN'), ('na', 'TO'), ('go', 'VB'), ('down', 'RP'), ('the', 'DT'), ('fire', 'NN'), ('escape', 'NN'), ('!', '.'), ('!', '.')]\n",
            "[(\"Y'know\", 'NNP'), (',', ','), ('here', 'RB'), (\"'s\", 'VBZ'), ('the', 'DT'), ('thing', 'NN'), ('.', '.'), ('Even', 'RB'), ('if', 'IN'), ('I', 'PRP'), ('could', 'MD'), ('get', 'VB'), ('it', 'PRP'), ('together', 'RB'), ('enough', 'RB'), ('to-', 'NN'), ('to', 'TO'), ('ask', 'VB'), ('a', 'DT'), ('woman', 'NN'), ('out', 'RP'), (',', ','), ('...', ':'), ('who', 'WP'), ('am', 'VBP'), ('I', 'PRP'), ('gon', 'VBP'), ('na', 'TO'), ('ask', 'VB'), ('?', '.')]\n",
            "[('Okay', 'NNP'), (',', ','), ('and', 'CC'), ('oh', 'UH'), ('I', 'PRP'), ('?', '.'), ('m', \"''\"), ('gon', 'NN'), ('na', 'TO'), ('need', 'VB'), ('a', 'DT'), ('bunch', 'NN'), ('of', 'IN'), ('extra', 'JJ'), ('keys', 'NNS'), ('.', '.'), ('Apparently', 'RB'), ('I', 'PRP'), ('give', 'VBP'), ('them', 'PRP'), ('away', 'RB'), ('for', 'IN'), ('no', 'DT'), ('reason', 'NN'), ('at', 'IN'), ('all', 'DT'), ('.', '.')]\n",
            "[('I', 'PRP'), ('remember', 'VBP'), ('the', 'DT'), ('moonlight', 'NN'), ('coming', 'VBG'), ('through', 'IN'), ('the', 'DT'), ('window-', 'JJ'), ('and', 'CC'), ('her', 'PRP$'), ('face', 'NN'), ('had', 'VBD'), ('the', 'DT'), ('most', 'RBS'), ('incredible', 'JJ'), ('glow', 'NN'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b28W3K8Pvbct"
      },
      "source": [
        "## 3) 개체명 인식"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHoDfJu2v_po",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc7fbf1b-f366-4d4a-cde7-6d2a3832f3c5"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "sentences=\"Y'know, ever since I ran out on Barry at the wedding, I have wondered whether I made the right choice.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "entities=nltk.chunk.ne_chunk(tagged)\n",
        "print(entities)\n",
        "\n",
        "sentences=\"I just went to your building and you weren't there and then this guy with a big hammer said you might be here and you are, you are!\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "entities=nltk.chunk.ne_chunk(tagged)\n",
        "print(entities)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "(S\n",
            "  Y'know/NN\n",
            "  ,/,\n",
            "  ever/RB\n",
            "  since/IN\n",
            "  I/PRP\n",
            "  ran/VBP\n",
            "  out/RP\n",
            "  on/IN\n",
            "  (PERSON Barry/NNP)\n",
            "  at/IN\n",
            "  the/DT\n",
            "  wedding/NN\n",
            "  ,/,\n",
            "  I/PRP\n",
            "  have/VBP\n",
            "  wondered/VBN\n",
            "  whether/IN\n",
            "  I/PRP\n",
            "  made/VBD\n",
            "  the/DT\n",
            "  right/JJ\n",
            "  choice/NN\n",
            "  ./.)\n",
            "(S\n",
            "  I/PRP\n",
            "  just/RB\n",
            "  went/VBD\n",
            "  to/TO\n",
            "  your/PRP$\n",
            "  building/NN\n",
            "  and/CC\n",
            "  you/PRP\n",
            "  were/VBD\n",
            "  n't/RB\n",
            "  there/RB\n",
            "  and/CC\n",
            "  then/RB\n",
            "  this/DT\n",
            "  guy/NN\n",
            "  with/IN\n",
            "  a/DT\n",
            "  big/JJ\n",
            "  hammer/NN\n",
            "  said/VBD\n",
            "  you/PRP\n",
            "  might/MD\n",
            "  be/VB\n",
            "  here/RB\n",
            "  and/CC\n",
            "  you/PRP\n",
            "  are/VBP\n",
            "  ,/,\n",
            "  you/PRP\n",
            "  are/VBP\n",
            "  !/.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEBQcfqMwzk1"
      },
      "source": [
        "## 4) Stemming : 어근 추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRbW6Xv9xOzi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "757550f9-5c9f-4797-c051-97747739e55c"
      },
      "source": [
        "import nltk \n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "pst=PorterStemmer()\n",
        "\n",
        "print(pst.stem(\"Y'know\"))\n",
        "print(pst.stem(\"ever\"))\n",
        "print(pst.stem(\"since\"))\n",
        "print(pst.stem(\"wondered\"))\n",
        "print(pst.stem(\"building\"))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y'know\n",
            "ever\n",
            "sinc\n",
            "wonder\n",
            "build\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIc7CdfRxSYM"
      },
      "source": [
        "## 5) Lemmatization : 원형 추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKxKMKpsxhJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37d55c93-5d73-4d37-da47-6c7c46cbec06"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "wlem=WordNetLemmatizer()\n",
        "\n",
        "print(wlem.lemmatize(\"ran\",pos='v'))\n",
        "print(wlem.lemmatize(\"have\",pos='v'))\n",
        "print(wlem.lemmatize(\"wondered\",pos='v'))\n",
        "print(wlem.lemmatize(\"went\",pos='v'))\n",
        "print(wlem.lemmatize(\"are\",pos='v'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "run\n",
            "have\n",
            "wonder\n",
            "go\n",
            "be\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0TOQb8txnLH"
      },
      "source": [
        "## 6) Stopword 제외 : 불용어 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPY5Sxdox6kF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2db7377f-744d-4d48-a895-690789a6826d"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "stop=set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "sen=\"Y'know, ever since I ran out on Barry at the wedding, I have wondered whether I made the right choice.\"\n",
        "tokens=nltk.word_tokenize(sen)\n",
        "\n",
        "#clean_tokens=[tok for tok in tokens if len(tok.lower())>1 and (tok.lower() not in stop)]\n",
        "\n",
        "clean_tokens=[]\n",
        "for tok in tokens:\n",
        "  if len(tok.lower())>1 and (tok.lower() not in stop):\n",
        "    clean_tokens.append(tok)\n",
        "\n",
        "  \n",
        "print(\"불용어 포함: \",tokens) \n",
        "print(\"불용어 미포함: \",clean_tokens)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "불용어 포함:  [\"Y'know\", ',', 'ever', 'since', 'I', 'ran', 'out', 'on', 'Barry', 'at', 'the', 'wedding', ',', 'I', 'have', 'wondered', 'whether', 'I', 'made', 'the', 'right', 'choice', '.']\n",
            "불용어 미포함:  [\"Y'know\", 'ever', 'since', 'ran', 'Barry', 'wedding', 'wondered', 'whether', 'made', 'right', 'choice']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqjkgwcwAXyt"
      },
      "source": [
        "## 2. Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "klpmalhZAcMC",
        "outputId": "9df6cf70-2d3d-41e1-ef88-7af7cfcccf18"
      },
      "source": [
        "from keras.datasets import imdb\r\n",
        "from keras import models\r\n",
        "from keras import layers\r\n",
        "import numpy as np\r\n",
        "from keras.utils import to_categorical\r\n",
        "\r\n",
        "(train_data,train_labels),(test_data,test_labels)=imdb.load_data(num_words=10000) #데이터 로딩 + 자주 나타나는 단어 1만개만 사용하겠다. => 적절한 크기의 벡터 데이터를 얻기 위함.\r\n",
        "  \r\n",
        "def make_one_hot(sequences,dimension=10000):\r\n",
        "  results=np.zeros((len(sequences),dimension)) #10000차원으로 개수만큼 \r\n",
        "\r\n",
        "  for i,sequence in enumerate(sequences):\r\n",
        "    results[i,sequence]=1. #results[i]에서 특정 인덱스의 위치를 1로 만듭니다\r\n",
        "  return results\r\n",
        "\r\n",
        "x_train=make_one_hot(train_data)\r\n",
        "x_test=make_one_hot(test_data)\r\n",
        "\r\n",
        "y_train=np.asarray(train_labels).astype('float32')\r\n",
        "y_test=np.asarray(test_labels).astype('float32')\r\n",
        "\r\n",
        "#모델 정의\r\n",
        "model=models.Sequential()\r\n",
        "model.add(layers.Dense(16,activation='relu',input_shape=(10000,)))\r\n",
        "model.add(layers.Dense(16,activation='relu'))\r\n",
        "model.add(layers.Dense(1,activation='sigmoid')) #긍정이냐 부정이냐 !\r\n",
        "\r\n",
        "#binary_crossentropy는 정보이론에서 온 개념으로 확률 분포간의 차이를 측정한다.(이진 분류에서 사용한다)\r\n",
        "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\r\n",
        "\r\n",
        "#또 다른 방법\r\n",
        "from keras import optimizers\r\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\r\n",
        "\r\n",
        "#또 다른 방법\r\n",
        "from keras import losses\r\n",
        "from keras import metrics\r\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001),loss=losses.binary_crossentropy,metrics=[metrics.binary_accuracy])\r\n",
        "\r\n",
        "#검증 데이터 만들기\r\n",
        "x_val=x_train[:10000]\r\n",
        "partial_x_train=x_train[10000:]\r\n",
        "\r\n",
        "y_val=y_train[:10000]\r\n",
        "partial_y_train=y_train[10000:]\r\n",
        "\r\n",
        "#훈련하기\r\n",
        "#model.fit이 History객체를 반환한다.\r\n",
        "#객체는 (acc,loss,val_acc,val_loss))를 딕셔너리 형태로 가지고 있다.\r\n",
        "#검증 데이터는 validation_data에 전달한다.\r\n",
        "history=model.fit(partial_x_train,partial_y_train,epochs=20,batch_size=512,validation_data=(x_val,y_val))\r\n",
        "\r\n",
        "#시각화 하기 - 훈련과 검증 손실\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "history_dict=history.history\r\n",
        "loss=history_dict['loss']\r\n",
        "val_loss=history_dict['val_loss']\r\n",
        "\r\n",
        "epochs=range(1,len(loss)+1)\r\n",
        "\r\n",
        "plt.plot(epochs,loss,'bo',label='Training loss')\r\n",
        "plt.plot(epochs,val_loss,'b',label='Validatin_loss')\r\n",
        "\r\n",
        "plt.title('Training and Validation Loss')\r\n",
        "plt.xlabel('Epochs')\r\n",
        "plt.ylabel('Loss')\r\n",
        "\r\n",
        "plt.legend()\r\n",
        "\r\n",
        "plt.show()\r\n",
        "\r\n",
        "\r\n",
        "#성능 측정\r\n",
        "results=model.evaluate(x_test,y_test)\r\n",
        "print(\"Results: \",results)\r\n",
        "\r\n",
        "from keras.models import load_model\r\n",
        "model.save('KU_NLP') #모델 저장하기"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "30/30 [==============================] - 2s 53ms/step - loss: 0.5903 - binary_accuracy: 0.7012 - val_loss: 0.3898 - val_binary_accuracy: 0.8757\n",
            "Epoch 2/20\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.3250 - binary_accuracy: 0.9078 - val_loss: 0.3232 - val_binary_accuracy: 0.8776\n",
            "Epoch 3/20\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.2386 - binary_accuracy: 0.9276 - val_loss: 0.2784 - val_binary_accuracy: 0.8916\n",
            "Epoch 4/20\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.1755 - binary_accuracy: 0.9491 - val_loss: 0.2765 - val_binary_accuracy: 0.8903\n",
            "Epoch 5/20\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.1451 - binary_accuracy: 0.9570 - val_loss: 0.3111 - val_binary_accuracy: 0.8741\n",
            "Epoch 6/20\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.1162 - binary_accuracy: 0.9680 - val_loss: 0.2895 - val_binary_accuracy: 0.8856\n",
            "Epoch 7/20\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.0933 - binary_accuracy: 0.9764 - val_loss: 0.3059 - val_binary_accuracy: 0.8834\n",
            "Epoch 8/20\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.0779 - binary_accuracy: 0.9822 - val_loss: 0.3262 - val_binary_accuracy: 0.8808\n",
            "Epoch 9/20\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.0662 - binary_accuracy: 0.9849 - val_loss: 0.3826 - val_binary_accuracy: 0.8696\n",
            "Epoch 10/20\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.0542 - binary_accuracy: 0.9877 - val_loss: 0.3966 - val_binary_accuracy: 0.8692\n",
            "Epoch 11/20\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.0424 - binary_accuracy: 0.9918 - val_loss: 0.4051 - val_binary_accuracy: 0.8720\n",
            "Epoch 12/20\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.0336 - binary_accuracy: 0.9939 - val_loss: 0.4259 - val_binary_accuracy: 0.8734\n",
            "Epoch 13/20\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.0252 - binary_accuracy: 0.9956 - val_loss: 0.4825 - val_binary_accuracy: 0.8716\n",
            "Epoch 14/20\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.0204 - binary_accuracy: 0.9973 - val_loss: 0.5386 - val_binary_accuracy: 0.8587\n",
            "Epoch 15/20\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.0165 - binary_accuracy: 0.9985 - val_loss: 0.5135 - val_binary_accuracy: 0.8701\n",
            "Epoch 16/20\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.0114 - binary_accuracy: 0.9989 - val_loss: 0.5447 - val_binary_accuracy: 0.8701\n",
            "Epoch 17/20\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.0085 - binary_accuracy: 0.9994 - val_loss: 0.5814 - val_binary_accuracy: 0.8647\n",
            "Epoch 18/20\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.0067 - binary_accuracy: 0.9996 - val_loss: 0.6102 - val_binary_accuracy: 0.8676\n",
            "Epoch 19/20\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 0.0050 - binary_accuracy: 0.9996 - val_loss: 0.7216 - val_binary_accuracy: 0.8605\n",
            "Epoch 20/20\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.0045 - binary_accuracy: 0.9999 - val_loss: 0.7135 - val_binary_accuracy: 0.8640\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7566 - binary_accuracy: 0.8489\n",
            "Results:  [0.7565682530403137, 0.8489199876785278]\n",
            "INFO:tensorflow:Assets written to: KU_NLP/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peK_mCVGFsmK",
        "outputId": "38c52631-5da5-494a-d25e-9719731630c8"
      },
      "source": [
        "from keras.datasets import mnist\r\n",
        "from keras import models\r\n",
        "from keras import layers\r\n",
        "from keras.utils import to_categorical\r\n",
        "\r\n",
        "#데이터 로드하기 , 학습데이터와 테스트 데이터\r\n",
        "(train_images,train_labels),(test_images,test_labels)= mnist.load_data() \r\n",
        "\r\n",
        "#데이터 전처리\r\n",
        "train_images=train_images.reshape((60000,28*28))\r\n",
        "train_images=train_images.astype('float32')/255\r\n",
        "\r\n",
        "test_images=test_images.reshape((10000,28*28))\r\n",
        "test_images=test_images.astype('float32')/255\r\n",
        "\r\n",
        "train_labels=to_categorical(train_labels)\r\n",
        "test_labels=to_categorical(test_labels)\r\n",
        "\r\n",
        "#모델 설계\r\n",
        "model=models.Sequential()\r\n",
        "model.add(layers.Dense(512,activation=\"relu\",input_shape=(28*28,)))\r\n",
        "model.add(layers.Dense(10,activation=\"softmax\"))\r\n",
        "\r\n",
        "#모델 컴파일\r\n",
        "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\r\n",
        "\r\n",
        "#모델 훈련\r\n",
        "model.fit(train_images,train_labels,epochs=5,batch_size=128)\r\n",
        "\r\n",
        "#정확도 측정\r\n",
        "test_loss,test_acc=model.evaluate(test_images,test_labels)\r\n",
        "print(\"정확도: \",test_acc)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 5s 9ms/step - loss: 0.4258 - accuracy: 0.8755\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.1090 - accuracy: 0.9681\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0684 - accuracy: 0.9800\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0469 - accuracy: 0.9858\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 4s 10ms/step - loss: 0.0375 - accuracy: 0.9894\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0703 - accuracy: 0.9786\n",
            "정확도:  0.978600025177002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiwJrXAcE96H"
      },
      "source": [
        "## 3. 패키지 선언"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "OEEPX8iEE-JI",
        "outputId": "f44f8275-ef79-4280-90fd-2110af1d5a6e"
      },
      "source": [
        "import os\r\n",
        "import numpy as np\r\n",
        "import nltk\r\n",
        "import random\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "def read_dataset(dataset_type):\r\n",
        "  max_seq_len = 0\r\n",
        "  with open(\"./%s.txt\" % dataset_type, \"r\", encoding=\"utf-8\") as fr_handle:\r\n",
        "    labels, sentences = [], []\r\n",
        "    for line in fr_handle:\r\n",
        "      if line.strip() == 0:\r\n",
        "        continue\r\n",
        "      label = line.split(' ')[0]\r\n",
        "      label = 0 if label == \"__label__1\" else 1 # 부정이면 0, 긍정이면 1\r\n",
        "\r\n",
        "      sentence = ' '.join(line.split(' ')[1:])\r\n",
        "      tokenized_sentence = nltk.word_tokenize(sentence) #단어 단위 분리\r\n",
        "      max_seq_len = max(max_seq_len, len(tokenized_sentence))#가장 긴 문장길이 구하기 for Padding\r\n",
        "\r\n",
        "      labels.append(label)\r\n",
        "      sentences.append(sentence)\r\n",
        "    \r\n",
        "    return labels, sentences, max_seq_len\r\n",
        "\r\n",
        "TRAIN_LABELS, TRAIN_SENTENCES, TRAIN_MAX_SEQ_LEN = read_dataset(\"train\") #학습데이터 읽기\r\n",
        "TEST_LABELS, TEST_SENTENCES, TEST_MAX_SEQ_LEN = read_dataset(\"test\") #테스트데이터 읽기\r\n",
        "MAX_SEQUENCE_LEN = max(TRAIN_MAX_SEQ_LEN, TEST_MAX_SEQ_LEN) #Train과 Test 전체에서 가장 긴 길이\r\n",
        "\r\n",
        "print(\"Train : \", len(TRAIN_SENTENCES))\r\n",
        "for train_label, train_sent in zip(TRAIN_LABELS, TRAIN_SENTENCES[0:10]):\r\n",
        "  print(train_label, ':' ,train_sent)\r\n",
        "\r\n",
        "print()\r\n",
        "print(\"Test : \", len(TEST_SENTENCES))\r\n",
        "for test_label, test_sent in zip(TEST_LABELS, TEST_SENTENCES[0:10]):\r\n",
        "  print(test_label, ':' ,test_sent)\r\n",
        "\r\n",
        "print(\"MAX_SEQUENCE_LEN\", MAX_SEQUENCE_LEN)\r\n",
        "with open(\"./vocab.txt\", \"r\", encoding=\"utf-8\") as vocab_handle:\r\n",
        "  VOCAB = [line.strip() for line in vocab_handle if len(line.strip()) > 0]\r\n",
        "  print(\"Total vocabulary\", VOCAB)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-ad4d912c9906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mTRAIN_LABELS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_SENTENCES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_MAX_SEQ_LEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#학습데이터 읽기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mTEST_LABELS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST_SENTENCES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST_MAX_SEQ_LEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#테스트데이터 읽기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mMAX_SEQUENCE_LEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_MAX_SEQ_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST_MAX_SEQ_LEN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Train과 Test 전체에서 가장 긴 길이\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-ad4d912c9906>\u001b[0m in \u001b[0;36mread_dataset\u001b[0;34m(dataset_type)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mmax_seq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./%s.txt\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdataset_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfr_handle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfr_handle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './train.txt'"
          ]
        }
      ]
    }
  ]
}