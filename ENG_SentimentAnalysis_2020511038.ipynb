{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ENG_SentimentAnalysis_2020511038.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shcho11/sentiment_analysis-202012-/blob/main/ENG_SentimentAnalysis_2020511038.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9TOC7HKrcu-"
      },
      "source": [
        "# Text preprocessing\n",
        "<br>\n",
        "제작자: Songhyun Cho (조송현) <br>\n",
        "소속: 컴퓨터정보통신대학원 빅데이터융합학과<br>\n",
        "학번 : 2020511038<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKSNGS9RgXMp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qFdeQTFr8l2"
      },
      "source": [
        "## 1. 어절분리\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhlrf7WWt6De"
      },
      "source": [
        "1) Rachel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC3rSTSjuGoo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b65c96e9-9a60-4ba5-d527-6263935a7d67"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentences=\"Y'know, ever since I ran out on Barry at the wedding, I have wondered whether I made the right choice.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"I just went to your building and you weren't there and then this guy with a big hammer said you might be here and you are, you are!\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"Ooh! Look! Look! Look! Look, there's Joey's picture! This is so exciting!\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"Ahh, yes, I will have a glass of the Merlot and uh,  he will have a white wine spritzer.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"Hey, time-out, umm, yeah, does the captain know that we?re moving?\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"I think, if it was a little colder in here I could see your nipples through that sweater.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"Hey Mon, what are you doing now? Wanna come see a movie with us?\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"Are you kidding? I'm trained for nothing! I was laughed out of twelve interviews today.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"You would be too if you found John and David boots on sale, fifty percent off!\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"They're my new 'I don't need a job, I don't need my parents, I've got great boots' boots!\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"Well hello! Welcome to Monica's. May I take your coat?\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"No, no, not at the moment, no, I'm not. Are you?\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"Right, yeah, I've heard that about cute doctors.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)\n",
        "\n",
        "sentences=\"Well, actually Gunther sent me. You?re not allowed to have cups out here, it?s a thing.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[\"Y'know\", ',', 'ever', 'since', 'I', 'ran', 'out', 'on', 'Barry', 'at', 'the', 'wedding', ',', 'I', 'have', 'wondered', 'whether', 'I', 'made', 'the', 'right', 'choice', '.']\n",
            "['I', 'just', 'went', 'to', 'your', 'building', 'and', 'you', 'were', \"n't\", 'there', 'and', 'then', 'this', 'guy', 'with', 'a', 'big', 'hammer', 'said', 'you', 'might', 'be', 'here', 'and', 'you', 'are', ',', 'you', 'are', '!']\n",
            "['Ooh', '!', 'Look', '!', 'Look', '!', 'Look', '!', 'Look', ',', 'there', \"'s\", 'Joey', \"'s\", 'picture', '!', 'This', 'is', 'so', 'exciting', '!']\n",
            "['Ahh', ',', 'yes', ',', 'I', 'will', 'have', 'a', 'glass', 'of', 'the', 'Merlot', 'and', 'uh', ',', 'he', 'will', 'have', 'a', 'white', 'wine', 'spritzer', '.']\n",
            "['Hey', ',', 'time-out', ',', 'umm', ',', 'yeah', ',', 'does', 'the', 'captain', 'know', 'that', 'we', '?', 're', 'moving', '?']\n",
            "['I', 'think', ',', 'if', 'it', 'was', 'a', 'little', 'colder', 'in', 'here', 'I', 'could', 'see', 'your', 'nipples', 'through', 'that', 'sweater', '.']\n",
            "['Hey', 'Mon', ',', 'what', 'are', 'you', 'doing', 'now', '?', 'Wan', 'na', 'come', 'see', 'a', 'movie', 'with', 'us', '?']\n",
            "['Are', 'you', 'kidding', '?', 'I', \"'m\", 'trained', 'for', 'nothing', '!', 'I', 'was', 'laughed', 'out', 'of', 'twelve', 'interviews', 'today', '.']\n",
            "['You', 'would', 'be', 'too', 'if', 'you', 'found', 'John', 'and', 'David', 'boots', 'on', 'sale', ',', 'fifty', 'percent', 'off', '!']\n",
            "['They', \"'re\", 'my', 'new', \"'I\", 'do', \"n't\", 'need', 'a', 'job', ',', 'I', 'do', \"n't\", 'need', 'my', 'parents', ',', 'I', \"'ve\", 'got', 'great', 'boots', \"'\", 'boots', '!']\n",
            "['Well', 'hello', '!', 'Welcome', 'to', 'Monica', \"'s\", '.', 'May', 'I', 'take', 'your', 'coat', '?']\n",
            "['No', ',', 'no', ',', 'not', 'at', 'the', 'moment', ',', 'no', ',', 'I', \"'m\", 'not', '.', 'Are', 'you', '?']\n",
            "['Right', ',', 'yeah', ',', 'I', \"'ve\", 'heard', 'that', 'about', 'cute', 'doctors', '.']\n",
            "['Well', ',', 'actually', 'Gunther', 'sent', 'me', '.', 'You', '?', 're', 'not', 'allowed', 'to', 'have', 'cups', 'out', 'here', ',', 'it', '?', 's', 'a', 'thing', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CuCWZAshI0J"
      },
      "source": [
        "2) Monica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjolYfDOhQ3L",
        "outputId": "7e7bea9e-20f5-4307-eb68-3aa6f6038cf7"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "sentences=\"Come on.  Hello?  I?m sorry you have the wrong number.   Okay, I?ll call you later dad. I love you.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Hello? No rejection? I got shot down at fat camp! Boy, kids are mean when they?re hungry.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"I mean, what are you gonna do, never going to talk to her again?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"I mean I know it?s weird, it?s awkward, but you gotta at least try.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Okay, everybody, this is Rachel, another Lincoln High survivor.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"This is everybody, this is Chandler, and Phoebe, and Joey, and- you remember my brother Ross?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Probably some y'know, European good-bye thing he picked up in London.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Well, the end table is wrong, The couch looks bizarre and don't even get me started on the refrigerator magnets.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"She?s a hooker! She?s a hooker! She?s a?  Hi! Uh, we spoke on the phone.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"I know this is going to sound unbelievably selfish, but, were you planning on bringing up the whole baby/lesbian thing?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Hey guys! Do you wanna look at the song list for the wedding?  Guys?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"I?m sorry. I?m sorry. I-I should probably leave you girls alone.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Hey Frannie, welcome back! How was Florida?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"You mean you know Paul like I know Paul?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['Come', 'on', '.', 'Hello', '?', 'I', '?', 'm', 'sorry', 'you', 'have', 'the', 'wrong', 'number', '.', 'Okay', ',', 'I', '?', 'll', 'call', 'you', 'later', 'dad', '.', 'I', 'love', 'you', '.']\n",
            "['Hello', '?', 'No', 'rejection', '?', 'I', 'got', 'shot', 'down', 'at', 'fat', 'camp', '!', 'Boy', ',', 'kids', 'are', 'mean', 'when', 'they', '?', 're', 'hungry', '.']\n",
            "['I', 'mean', ',', 'what', 'are', 'you', 'gon', 'na', 'do', ',', 'never', 'going', 'to', 'talk', 'to', 'her', 'again', '?']\n",
            "['I', 'mean', 'I', 'know', 'it', '?', 's', 'weird', ',', 'it', '?', 's', 'awkward', ',', 'but', 'you', 'got', 'ta', 'at', 'least', 'try', '.']\n",
            "['Okay', ',', 'everybody', ',', 'this', 'is', 'Rachel', ',', 'another', 'Lincoln', 'High', 'survivor', '.']\n",
            "['This', 'is', 'everybody', ',', 'this', 'is', 'Chandler', ',', 'and', 'Phoebe', ',', 'and', 'Joey', ',', 'and-', 'you', 'remember', 'my', 'brother', 'Ross', '?']\n",
            "['Probably', 'some', \"y'know\", ',', 'European', 'good-bye', 'thing', 'he', 'picked', 'up', 'in', 'London', '.']\n",
            "['Well', ',', 'the', 'end', 'table', 'is', 'wrong', ',', 'The', 'couch', 'looks', 'bizarre', 'and', 'do', \"n't\", 'even', 'get', 'me', 'started', 'on', 'the', 'refrigerator', 'magnets', '.']\n",
            "['She', '?', 's', 'a', 'hooker', '!', 'She', '?', 's', 'a', 'hooker', '!', 'She', '?', 's', 'a', '?', 'Hi', '!', 'Uh', ',', 'we', 'spoke', 'on', 'the', 'phone', '.']\n",
            "['I', 'know', 'this', 'is', 'going', 'to', 'sound', 'unbelievably', 'selfish', ',', 'but', ',', 'were', 'you', 'planning', 'on', 'bringing', 'up', 'the', 'whole', 'baby/lesbian', 'thing', '?']\n",
            "['Hey', 'guys', '!', 'Do', 'you', 'wan', 'na', 'look', 'at', 'the', 'song', 'list', 'for', 'the', 'wedding', '?', 'Guys', '?']\n",
            "['I', '?', 'm', 'sorry', '.', 'I', '?', 'm', 'sorry', '.', 'I-I', 'should', 'probably', 'leave', 'you', 'girls', 'alone', '.']\n",
            "['Hey', 'Frannie', ',', 'welcome', 'back', '!', 'How', 'was', 'Florida', '?']\n",
            "['You', 'mean', 'you', 'know', 'Paul', 'like', 'I', 'know', 'Paul', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHR0rfF9if-E"
      },
      "source": [
        "3) Chandler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHY-GsGdivC3",
        "outputId": "34ebe361-0c00-4d8f-902a-aa28da0b7620"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "sentences=\"I don?t want him to tell this story for years.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"All right! Go left! Go left! Go right!! Go right!!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"So what do you say, maybe sometime I hold your gun?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"You can always spot someone who's never seen one of his plays before. Notice, no fear, no sense of impending doom...\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"I got caught up and work, but I'm quitting tomorrow.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Oh, why not. Was I doing anything particularly... saucy?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"No, I don't see anything different other than the fact that the room got so much brighter when you came into it.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Yeah, I miss that too. I tell you what; from now on we?ll make time to hang out with each other.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"I really shouldn?t have said that you were embarrassing me, I mean that really wasn?t cool.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"And if it makes you feel any better, I?ve had a really lousy day.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Oh, y'know, what did you mean when you said pivot?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Yes, the moon, the glow, the magical feeling, you did this part- Could I get some painkillers over here, please?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['I', 'don', '?', 't', 'want', 'him', 'to', 'tell', 'this', 'story', 'for', 'years', '.']\n",
            "['All', 'right', '!', 'Go', 'left', '!', 'Go', 'left', '!', 'Go', 'right', '!', '!', 'Go', 'right', '!', '!']\n",
            "['So', 'what', 'do', 'you', 'say', ',', 'maybe', 'sometime', 'I', 'hold', 'your', 'gun', '?']\n",
            "['You', 'can', 'always', 'spot', 'someone', 'who', \"'s\", 'never', 'seen', 'one', 'of', 'his', 'plays', 'before', '.', 'Notice', ',', 'no', 'fear', ',', 'no', 'sense', 'of', 'impending', 'doom', '...']\n",
            "['I', 'got', 'caught', 'up', 'and', 'work', ',', 'but', 'I', \"'m\", 'quitting', 'tomorrow', '.']\n",
            "['Oh', ',', 'why', 'not', '.', 'Was', 'I', 'doing', 'anything', 'particularly', '...', 'saucy', '?']\n",
            "['No', ',', 'I', 'do', \"n't\", 'see', 'anything', 'different', 'other', 'than', 'the', 'fact', 'that', 'the', 'room', 'got', 'so', 'much', 'brighter', 'when', 'you', 'came', 'into', 'it', '.']\n",
            "['Yeah', ',', 'I', 'miss', 'that', 'too', '.', 'I', 'tell', 'you', 'what', ';', 'from', 'now', 'on', 'we', '?', 'll', 'make', 'time', 'to', 'hang', 'out', 'with', 'each', 'other', '.']\n",
            "['I', 'really', 'shouldn', '?', 't', 'have', 'said', 'that', 'you', 'were', 'embarrassing', 'me', ',', 'I', 'mean', 'that', 'really', 'wasn', '?', 't', 'cool', '.']\n",
            "['And', 'if', 'it', 'makes', 'you', 'feel', 'any', 'better', ',', 'I', '?', 've', 'had', 'a', 'really', 'lousy', 'day', '.']\n",
            "['Oh', ',', \"y'know\", ',', 'what', 'did', 'you', 'mean', 'when', 'you', 'said', 'pivot', '?']\n",
            "['Yes', ',', 'the', 'moon', ',', 'the', 'glow', ',', 'the', 'magical', 'feeling', ',', 'you', 'did', 'this', 'part-', 'Could', 'I', 'get', 'some', 'painkillers', 'over', 'here', ',', 'please', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkFCLtjzjtqh"
      },
      "source": [
        "4) Joey"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVcb-dHAjvzU",
        "outputId": "5370dc4f-698d-4b00-92fc-d0323b162e1d"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "sentences=\"Then you gotta come clean with Ma! This is not right!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Listen, uh, what do you say I buy you that cup of coffee now?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Whoa-whoa! No-no-no-no-no, nothing is going up! Okay? Up, up is not an option?what's a urethra?  Are you crazy?!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Jo?s there, but I don?t think there?s anything she could do.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Oh thanks. Thanks. It was great meetin? ya. And listen if any of my friends gets married, or have a birthday, or a Tuesday\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Well, Chandler's old roomate was Jewish, and these are the only candles we have, so... Happy Chanukah, everyone.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Vell, Eva, ve've done some excellent vork here, and I vould have to say, your pwoblem is qviiite clear.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Oh sure?And hey, don?t get me wrong, I am so happy for you guys. I just?I miss?hanging out?just-just us, y?know?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"He seemed like a stand up guy. Oh, and he?s not into anything weird sexually.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"All right well, y?know?I guess we know what we have to do to get down.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Ross. I was thinking we could just go down the fire escape.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Noo!! I?ve had the best day ever! Dude, check this out!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Great story!? But, I uh, I gotta go, I got a date with Andrea--Angela--Andrea...? Oh man,\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['Then', 'you', 'got', 'ta', 'come', 'clean', 'with', 'Ma', '!', 'This', 'is', 'not', 'right', '!']\n",
            "['Listen', ',', 'uh', ',', 'what', 'do', 'you', 'say', 'I', 'buy', 'you', 'that', 'cup', 'of', 'coffee', 'now', '?']\n",
            "['Whoa-whoa', '!', 'No-no-no-no-no', ',', 'nothing', 'is', 'going', 'up', '!', 'Okay', '?', 'Up', ',', 'up', 'is', 'not', 'an', 'option', '?', 'what', \"'s\", 'a', 'urethra', '?', 'Are', 'you', 'crazy', '?', '!']\n",
            "['Jo', '?', 's', 'there', ',', 'but', 'I', 'don', '?', 't', 'think', 'there', '?', 's', 'anything', 'she', 'could', 'do', '.']\n",
            "['Oh', 'thanks', '.', 'Thanks', '.', 'It', 'was', 'great', 'meetin', '?', 'ya', '.', 'And', 'listen', 'if', 'any', 'of', 'my', 'friends', 'gets', 'married', ',', 'or', 'have', 'a', 'birthday', ',', 'or', 'a', 'Tuesday']\n",
            "['Well', ',', 'Chandler', \"'s\", 'old', 'roomate', 'was', 'Jewish', ',', 'and', 'these', 'are', 'the', 'only', 'candles', 'we', 'have', ',', 'so', '...', 'Happy', 'Chanukah', ',', 'everyone', '.']\n",
            "['Vell', ',', 'Eva', ',', 've', \"'ve\", 'done', 'some', 'excellent', 'vork', 'here', ',', 'and', 'I', 'vould', 'have', 'to', 'say', ',', 'your', 'pwoblem', 'is', 'qviiite', 'clear', '.']\n",
            "['Oh', 'sure', '?', 'And', 'hey', ',', 'don', '?', 't', 'get', 'me', 'wrong', ',', 'I', 'am', 'so', 'happy', 'for', 'you', 'guys', '.', 'I', 'just', '?', 'I', 'miss', '?', 'hanging', 'out', '?', 'just-just', 'us', ',', 'y', '?', 'know', '?']\n",
            "['He', 'seemed', 'like', 'a', 'stand', 'up', 'guy', '.', 'Oh', ',', 'and', 'he', '?', 's', 'not', 'into', 'anything', 'weird', 'sexually', '.']\n",
            "['All', 'right', 'well', ',', 'y', '?', 'know', '?', 'I', 'guess', 'we', 'know', 'what', 'we', 'have', 'to', 'do', 'to', 'get', 'down', '.']\n",
            "['Ross', '.', 'I', 'was', 'thinking', 'we', 'could', 'just', 'go', 'down', 'the', 'fire', 'escape', '.']\n",
            "['Noo', '!', '!', 'I', '?', 've', 'had', 'the', 'best', 'day', 'ever', '!', 'Dude', ',', 'check', 'this', 'out', '!']\n",
            "['Great', 'story', '!', '?', 'But', ',', 'I', 'uh', ',', 'I', 'got', 'ta', 'go', ',', 'I', 'got', 'a', 'date', 'with', 'Andrea', '--', 'Angela', '--', 'Andrea', '...', '?', 'Oh', 'man', ',']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0xuisF6nJQx"
      },
      "source": [
        "5) Ross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WwlMcQqnNqY",
        "outputId": "209bb0f5-cb34-481f-ff7f-86a8bf80b974"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "sentences=\"Oh, but he will. He still tells the story how Monica tried to escape from fat camp.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Okay, Andre should be there in like 45 minutes. All rightie, bye bye.  Just easier that way.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"The data we are receiving from MRI scans and DNA testing of these fossils are - are staggering.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"I mean, we've been accepting Leakey's dates as a given, but if they're off by even a hundred thousand years or so then you can - you can just throw most of our assumptions, you know, right in the trash.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"So-so what I am saying is - is is that  is that the repercussions could be huge!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"I mean, not just in palaeontology, but if-if you think about it, in evolutionary biology, uh, genetics, geology, uh, I mean, truly the mind boggles!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"You're right, you're right, it is...So you gonna invite us all to the big opening?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Oh, we were helping Chandler write his vows, but he kicked us out because Joey kept making inappropriate suggestions.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Yeah, laugh all you want but in ten minutes we?re gonna have younger looking skin!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Yeah, I guess we don?t have a choice.  Help us! Please help us! We?re stuck up on the roof and we can?t get down!!!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"I know, I wasn?t finished.   But don?t worry! We?re gonna go down the fire escape!!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Y'know, here's the thing. Even if I could get it together enough to- to ask a woman out,... who am I gonna ask?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"Okay, and oh I?m gonna need a bunch of extra keys. Apparently I give them away for no reason at all.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "sentences=\"I remember the moonlight coming through the window- and her face had the most incredible glow.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "print(tokens)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['Oh', ',', 'but', 'he', 'will', '.', 'He', 'still', 'tells', 'the', 'story', 'how', 'Monica', 'tried', 'to', 'escape', 'from', 'fat', 'camp', '.']\n",
            "['Okay', ',', 'Andre', 'should', 'be', 'there', 'in', 'like', '45', 'minutes', '.', 'All', 'rightie', ',', 'bye', 'bye', '.', 'Just', 'easier', 'that', 'way', '.']\n",
            "['The', 'data', 'we', 'are', 'receiving', 'from', 'MRI', 'scans', 'and', 'DNA', 'testing', 'of', 'these', 'fossils', 'are', '-', 'are', 'staggering', '.']\n",
            "['I', 'mean', ',', 'we', \"'ve\", 'been', 'accepting', 'Leakey', \"'s\", 'dates', 'as', 'a', 'given', ',', 'but', 'if', 'they', \"'re\", 'off', 'by', 'even', 'a', 'hundred', 'thousand', 'years', 'or', 'so', 'then', 'you', 'can', '-', 'you', 'can', 'just', 'throw', 'most', 'of', 'our', 'assumptions', ',', 'you', 'know', ',', 'right', 'in', 'the', 'trash', '.']\n",
            "['So-so', 'what', 'I', 'am', 'saying', 'is', '-', 'is', 'is', 'that', 'is', 'that', 'the', 'repercussions', 'could', 'be', 'huge', '!']\n",
            "['I', 'mean', ',', 'not', 'just', 'in', 'palaeontology', ',', 'but', 'if-if', 'you', 'think', 'about', 'it', ',', 'in', 'evolutionary', 'biology', ',', 'uh', ',', 'genetics', ',', 'geology', ',', 'uh', ',', 'I', 'mean', ',', 'truly', 'the', 'mind', 'boggles', '!']\n",
            "['You', \"'re\", 'right', ',', 'you', \"'re\", 'right', ',', 'it', 'is', '...', 'So', 'you', 'gon', 'na', 'invite', 'us', 'all', 'to', 'the', 'big', 'opening', '?']\n",
            "['Oh', ',', 'we', 'were', 'helping', 'Chandler', 'write', 'his', 'vows', ',', 'but', 'he', 'kicked', 'us', 'out', 'because', 'Joey', 'kept', 'making', 'inappropriate', 'suggestions', '.']\n",
            "['Yeah', ',', 'laugh', 'all', 'you', 'want', 'but', 'in', 'ten', 'minutes', 'we', '?', 're', 'gon', 'na', 'have', 'younger', 'looking', 'skin', '!']\n",
            "['Yeah', ',', 'I', 'guess', 'we', 'don', '?', 't', 'have', 'a', 'choice', '.', 'Help', 'us', '!', 'Please', 'help', 'us', '!', 'We', '?', 're', 'stuck', 'up', 'on', 'the', 'roof', 'and', 'we', 'can', '?', 't', 'get', 'down', '!', '!', '!']\n",
            "['I', 'know', ',', 'I', 'wasn', '?', 't', 'finished', '.', 'But', 'don', '?', 't', 'worry', '!', 'We', '?', 're', 'gon', 'na', 'go', 'down', 'the', 'fire', 'escape', '!', '!']\n",
            "[\"Y'know\", ',', 'here', \"'s\", 'the', 'thing', '.', 'Even', 'if', 'I', 'could', 'get', 'it', 'together', 'enough', 'to-', 'to', 'ask', 'a', 'woman', 'out', ',', '...', 'who', 'am', 'I', 'gon', 'na', 'ask', '?']\n",
            "['Okay', ',', 'and', 'oh', 'I', '?', 'm', 'gon', 'na', 'need', 'a', 'bunch', 'of', 'extra', 'keys', '.', 'Apparently', 'I', 'give', 'them', 'away', 'for', 'no', 'reason', 'at', 'all', '.']\n",
            "['I', 'remember', 'the', 'moonlight', 'coming', 'through', 'the', 'window-', 'and', 'her', 'face', 'had', 'the', 'most', 'incredible', 'glow', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3dtjkc_vEPK"
      },
      "source": [
        "##2. 형태소 분석"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUk0d9dxp6oH"
      },
      "source": [
        "1) Rachel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlSkeHw2vQqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e68d643-3d60-4ed0-9318-3ff81aceb5fa"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sentences=\"Y'know, ever since I ran out on Barry at the wedding, I have wondered whether I made the right choice.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"I just went to your building and you weren't there and then this guy with a big hammer said you might be here and you are, you are!\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"Ooh! Look! Look! Look! Look, there's Joey's picture! This is so exciting!\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"Ahh, yes, I will have a glass of the Merlot and uh,  he will have a white wine spritzer.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"Hey, time-out, umm, yeah, does the captain know that we?re moving?\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"I think, if it was a little colder in here I could see your nipples through that sweater.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"Hey Mon, what are you doing now? Wanna come see a movie with us?\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"Are you kidding? I'm trained for nothing! I was laughed out of twelve interviews today.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"You would be too if you found John and David boots on sale, fifty percent off!\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"They're my new 'I don't need a job, I don't need my parents, I've got great boots' boots!\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"Well hello! Welcome to Monica's. May I take your coat?\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"No, no, not at the moment, no, I'm not. Are you?\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"Right, yeah, I've heard that about cute doctors.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "sentences=\"Well, actually Gunther sent me. You?re not allowed to have cups out here, it?s a thing.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "print(tagged)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[(\"Y'know\", 'NN'), (',', ','), ('ever', 'RB'), ('since', 'IN'), ('I', 'PRP'), ('ran', 'VBP'), ('out', 'RP'), ('on', 'IN'), ('Barry', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('wedding', 'NN'), (',', ','), ('I', 'PRP'), ('have', 'VBP'), ('wondered', 'VBN'), ('whether', 'IN'), ('I', 'PRP'), ('made', 'VBD'), ('the', 'DT'), ('right', 'JJ'), ('choice', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('just', 'RB'), ('went', 'VBD'), ('to', 'TO'), ('your', 'PRP$'), ('building', 'NN'), ('and', 'CC'), ('you', 'PRP'), ('were', 'VBD'), (\"n't\", 'RB'), ('there', 'RB'), ('and', 'CC'), ('then', 'RB'), ('this', 'DT'), ('guy', 'NN'), ('with', 'IN'), ('a', 'DT'), ('big', 'JJ'), ('hammer', 'NN'), ('said', 'VBD'), ('you', 'PRP'), ('might', 'MD'), ('be', 'VB'), ('here', 'RB'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), (',', ','), ('you', 'PRP'), ('are', 'VBP'), ('!', '.')]\n",
            "[('Ooh', 'NN'), ('!', '.'), ('Look', 'NN'), ('!', '.'), ('Look', 'NN'), ('!', '.'), ('Look', 'NN'), ('!', '.'), ('Look', 'NNP'), (',', ','), ('there', 'EX'), (\"'s\", 'VBZ'), ('Joey', 'NNP'), (\"'s\", 'POS'), ('picture', 'NN'), ('!', '.'), ('This', 'DT'), ('is', 'VBZ'), ('so', 'RB'), ('exciting', 'JJ'), ('!', '.')]\n",
            "[('Ahh', 'NNP'), (',', ','), ('yes', 'UH'), (',', ','), ('I', 'PRP'), ('will', 'MD'), ('have', 'VB'), ('a', 'DT'), ('glass', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Merlot', 'NNP'), ('and', 'CC'), ('uh', 'UH'), (',', ','), ('he', 'PRP'), ('will', 'MD'), ('have', 'VB'), ('a', 'DT'), ('white', 'JJ'), ('wine', 'NN'), ('spritzer', 'NN'), ('.', '.')]\n",
            "[('Hey', 'NNP'), (',', ','), ('time-out', 'NN'), (',', ','), ('umm', 'JJ'), (',', ','), ('yeah', 'UH'), (',', ','), ('does', 'VBZ'), ('the', 'DT'), ('captain', 'NN'), ('know', 'VBP'), ('that', 'IN'), ('we', 'PRP'), ('?', '.'), ('re', 'VB'), ('moving', 'VBG'), ('?', '.')]\n",
            "[('I', 'PRP'), ('think', 'VBP'), (',', ','), ('if', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('a', 'DT'), ('little', 'JJ'), ('colder', 'NN'), ('in', 'IN'), ('here', 'RB'), ('I', 'PRP'), ('could', 'MD'), ('see', 'VB'), ('your', 'PRP$'), ('nipples', 'NNS'), ('through', 'IN'), ('that', 'DT'), ('sweater', 'NN'), ('.', '.')]\n",
            "[('Hey', 'NNP'), ('Mon', 'NNP'), (',', ','), ('what', 'WP'), ('are', 'VBP'), ('you', 'PRP'), ('doing', 'VBG'), ('now', 'RB'), ('?', '.'), ('Wan', 'NNP'), ('na', 'CC'), ('come', 'VBN'), ('see', 'VBP'), ('a', 'DT'), ('movie', 'NN'), ('with', 'IN'), ('us', 'PRP'), ('?', '.')]\n",
            "[('Are', 'NNP'), ('you', 'PRP'), ('kidding', 'VBG'), ('?', '.'), ('I', 'PRP'), (\"'m\", 'VBP'), ('trained', 'JJ'), ('for', 'IN'), ('nothing', 'NN'), ('!', '.'), ('I', 'PRP'), ('was', 'VBD'), ('laughed', 'VBN'), ('out', 'IN'), ('of', 'IN'), ('twelve', 'NN'), ('interviews', 'NNS'), ('today', 'NN'), ('.', '.')]\n",
            "[('You', 'PRP'), ('would', 'MD'), ('be', 'VB'), ('too', 'RB'), ('if', 'IN'), ('you', 'PRP'), ('found', 'VBP'), ('John', 'NNP'), ('and', 'CC'), ('David', 'NNP'), ('boots', 'NNS'), ('on', 'IN'), ('sale', 'NN'), (',', ','), ('fifty', 'JJ'), ('percent', 'NN'), ('off', 'IN'), ('!', '.')]\n",
            "[('They', 'PRP'), (\"'re\", 'VBP'), ('my', 'PRP$'), ('new', 'JJ'), (\"'I\", 'NNS'), ('do', 'VBP'), (\"n't\", 'RB'), ('need', 'VB'), ('a', 'DT'), ('job', 'NN'), (',', ','), ('I', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('need', 'VB'), ('my', 'PRP$'), ('parents', 'NNS'), (',', ','), ('I', 'PRP'), (\"'ve\", 'VBP'), ('got', 'VBN'), ('great', 'JJ'), ('boots', 'NNS'), (\"'\", 'POS'), ('boots', 'NNS'), ('!', '.')]\n",
            "[('Well', 'RB'), ('hello', 'RB'), ('!', '.'), ('Welcome', 'NNP'), ('to', 'TO'), ('Monica', 'NNP'), (\"'s\", 'POS'), ('.', '.'), ('May', 'NNP'), ('I', 'PRP'), ('take', 'VBP'), ('your', 'PRP$'), ('coat', 'NN'), ('?', '.')]\n",
            "[('No', 'DT'), (',', ','), ('no', 'DT'), (',', ','), ('not', 'RB'), ('at', 'IN'), ('the', 'DT'), ('moment', 'NN'), (',', ','), ('no', 'DT'), (',', ','), ('I', 'PRP'), (\"'m\", 'VBP'), ('not', 'RB'), ('.', '.'), ('Are', 'VB'), ('you', 'PRP'), ('?', '.')]\n",
            "[('Right', 'RB'), (',', ','), ('yeah', 'UH'), (',', ','), ('I', 'PRP'), (\"'ve\", 'VBP'), ('heard', 'VBN'), ('that', 'IN'), ('about', 'IN'), ('cute', 'NN'), ('doctors', 'NNS'), ('.', '.')]\n",
            "[('Well', 'RB'), (',', ','), ('actually', 'RB'), ('Gunther', 'NNP'), ('sent', 'VBD'), ('me', 'PRP'), ('.', '.'), ('You', 'PRP'), ('?', '.'), ('re', 'VB'), ('not', 'RB'), ('allowed', 'VBN'), ('to', 'TO'), ('have', 'VB'), ('cups', 'NNS'), ('out', 'RP'), ('here', 'RB'), (',', ','), ('it', 'PRP'), ('?', '.'), ('s', 'VBZ'), ('a', 'DT'), ('thing', 'NN'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aH_tRvFw7df"
      },
      "source": [
        "2) Monica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMmLL92Yw7ln",
        "outputId": "72a13077-e909-48d0-bd19-dedd6910b250"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "\r\n",
        "sentences=\"Come on.  Hello?  I?m sorry you have the wrong number.   Okay, I?ll call you later dad. I love you.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Hello? No rejection? I got shot down at fat camp! Boy, kids are mean when they?re hungry.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"I mean, what are you gonna do, never going to talk to her again?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"I mean I know it?s weird, it?s awkward, but you gotta at least try.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Okay, everybody, this is Rachel, another Lincoln High survivor.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"This is everybody, this is Chandler, and Phoebe, and Joey, and- you remember my brother Ross?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Probably some y'know, European good-bye thing he picked up in London.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Well, the end table is wrong, The couch looks bizarre and don't even get me started on the refrigerator magnets.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"She?s a hooker! She?s a hooker! She?s a?  Hi! Uh, we spoke on the phone.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"I know this is going to sound unbelievably selfish, but, were you planning on bringing up the whole baby/lesbian thing?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Hey guys! Do you wanna look at the song list for the wedding?  Guys?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"I?m sorry. I?m sorry. I-I should probably leave you girls alone.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Hey Frannie, welcome back! How was Florida?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"You mean you know Paul like I know Paul?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[('Come', 'VBN'), ('on', 'IN'), ('.', '.'), ('Hello', 'NNP'), ('?', '.'), ('I', 'PRP'), ('?', '.'), ('m', 'NN'), ('sorry', 'NN'), ('you', 'PRP'), ('have', 'VBP'), ('the', 'DT'), ('wrong', 'JJ'), ('number', 'NN'), ('.', '.'), ('Okay', 'NNP'), (',', ','), ('I', 'PRP'), ('?', '.'), ('ll', \"''\"), ('call', 'NN'), ('you', 'PRP'), ('later', 'RB'), ('dad', 'VBP'), ('.', '.'), ('I', 'PRP'), ('love', 'VBP'), ('you', 'PRP'), ('.', '.')]\n",
            "[('Hello', 'NNP'), ('?', '.'), ('No', 'DT'), ('rejection', 'NN'), ('?', '.'), ('I', 'PRP'), ('got', 'VBD'), ('shot', 'RB'), ('down', 'RB'), ('at', 'IN'), ('fat', 'JJ'), ('camp', 'NN'), ('!', '.'), ('Boy', 'NNP'), (',', ','), ('kids', 'NNS'), ('are', 'VBP'), ('mean', 'JJ'), ('when', 'WRB'), ('they', 'PRP'), ('?', '.'), ('re', 'NN'), ('hungry', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('mean', 'VBP'), (',', ','), ('what', 'WP'), ('are', 'VBP'), ('you', 'PRP'), ('gon', 'VB'), ('na', 'TO'), ('do', 'VB'), (',', ','), ('never', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('talk', 'VB'), ('to', 'TO'), ('her', 'PRP$'), ('again', 'RB'), ('?', '.')]\n",
            "[('I', 'PRP'), ('mean', 'VBP'), ('I', 'PRP'), ('know', 'VBP'), ('it', 'PRP'), ('?', '.'), ('s', 'JJ'), ('weird', 'NN'), (',', ','), ('it', 'PRP'), ('?', '.'), ('s', 'JJ'), ('awkward', 'NN'), (',', ','), ('but', 'CC'), ('you', 'PRP'), ('got', 'VBD'), ('ta', 'NNS'), ('at', 'IN'), ('least', 'JJS'), ('try', 'NN'), ('.', '.')]\n",
            "[('Okay', 'NNP'), (',', ','), ('everybody', 'NN'), (',', ','), ('this', 'DT'), ('is', 'VBZ'), ('Rachel', 'NNP'), (',', ','), ('another', 'DT'), ('Lincoln', 'NNP'), ('High', 'NNP'), ('survivor', 'NN'), ('.', '.')]\n",
            "[('This', 'DT'), ('is', 'VBZ'), ('everybody', 'NN'), (',', ','), ('this', 'DT'), ('is', 'VBZ'), ('Chandler', 'NNP'), (',', ','), ('and', 'CC'), ('Phoebe', 'NNP'), (',', ','), ('and', 'CC'), ('Joey', 'NNP'), (',', ','), ('and-', 'NN'), ('you', 'PRP'), ('remember', 'VBP'), ('my', 'PRP$'), ('brother', 'NN'), ('Ross', 'NNP'), ('?', '.')]\n",
            "[('Probably', 'RB'), ('some', 'DT'), (\"y'know\", 'NN'), (',', ','), ('European', 'JJ'), ('good-bye', 'JJ'), ('thing', 'NN'), ('he', 'PRP'), ('picked', 'VBD'), ('up', 'RP'), ('in', 'IN'), ('London', 'NNP'), ('.', '.')]\n",
            "[('Well', 'RB'), (',', ','), ('the', 'DT'), ('end', 'NN'), ('table', 'NN'), ('is', 'VBZ'), ('wrong', 'JJ'), (',', ','), ('The', 'DT'), ('couch', 'JJ'), ('looks', 'NNS'), ('bizarre', 'JJ'), ('and', 'CC'), ('do', 'VBP'), (\"n't\", 'RB'), ('even', 'RB'), ('get', 'VB'), ('me', 'PRP'), ('started', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('refrigerator', 'NN'), ('magnets', 'NNS'), ('.', '.')]\n",
            "[('She', 'PRP'), ('?', '.'), ('s', 'VB'), ('a', 'DT'), ('hooker', 'NN'), ('!', '.'), ('She', 'PRP'), ('?', '.'), ('s', 'VB'), ('a', 'DT'), ('hooker', 'NN'), ('!', '.'), ('She', 'PRP'), ('?', '.'), ('s', 'VB'), ('a', 'DT'), ('?', '.'), ('Hi', 'NN'), ('!', '.'), ('Uh', 'NNP'), (',', ','), ('we', 'PRP'), ('spoke', 'VBD'), ('on', 'IN'), ('the', 'DT'), ('phone', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('know', 'VBP'), ('this', 'DT'), ('is', 'VBZ'), ('going', 'VBG'), ('to', 'TO'), ('sound', 'VB'), ('unbelievably', 'RB'), ('selfish', 'JJ'), (',', ','), ('but', 'CC'), (',', ','), ('were', 'VBD'), ('you', 'PRP'), ('planning', 'VBG'), ('on', 'IN'), ('bringing', 'VBG'), ('up', 'RP'), ('the', 'DT'), ('whole', 'JJ'), ('baby/lesbian', 'JJ'), ('thing', 'NN'), ('?', '.')]\n",
            "[('Hey', 'NNP'), ('guys', 'NNS'), ('!', '.'), ('Do', 'VBP'), ('you', 'PRP'), ('wan', 'VB'), ('na', 'JJ'), ('look', 'NN'), ('at', 'IN'), ('the', 'DT'), ('song', 'JJ'), ('list', 'NN'), ('for', 'IN'), ('the', 'DT'), ('wedding', 'NN'), ('?', '.'), ('Guys', 'NNP'), ('?', '.')]\n",
            "[('I', 'PRP'), ('?', '.'), ('m', 'NN'), ('sorry', 'NN'), ('.', '.'), ('I', 'PRP'), ('?', '.'), ('m', 'NN'), ('sorry', 'NN'), ('.', '.'), ('I-I', 'NNP'), ('should', 'MD'), ('probably', 'RB'), ('leave', 'VB'), ('you', 'PRP'), ('girls', 'VB'), ('alone', 'RB'), ('.', '.')]\n",
            "[('Hey', 'NNP'), ('Frannie', 'NNP'), (',', ','), ('welcome', 'VB'), ('back', 'RB'), ('!', '.'), ('How', 'NN'), ('was', 'VBD'), ('Florida', 'NNP'), ('?', '.')]\n",
            "[('You', 'PRP'), ('mean', 'VBP'), ('you', 'PRP'), ('know', 'VBP'), ('Paul', 'NNP'), ('like', 'IN'), ('I', 'PRP'), ('know', 'VBP'), ('Paul', 'NNP'), ('?', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgAKRWyaxr8E"
      },
      "source": [
        "3) Chandler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOG6PoUnxjwT",
        "outputId": "2e8ae1ba-ea7f-4384-db9b-d4db7efbef5e"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "\r\n",
        "sentences=\"I don?t want him to tell this story for years.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"All right! Go left! Go left! Go right!! Go right!!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"So what do you say, maybe sometime I hold your gun?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"You can always spot someone who's never seen one of his plays before. Notice, no fear, no sense of impending doom...\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"I got caught up and work, but I'm quitting tomorrow.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Oh, why not. Was I doing anything particularly... saucy?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"No, I don't see anything different other than the fact that the room got so much brighter when you came into it.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Yeah, I miss that too. I tell you what; from now on we?ll make time to hang out with each other.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"I really shouldn?t have said that you were embarrassing me, I mean that really wasn?t cool.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"And if it makes you feel any better, I?ve had a really lousy day.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Oh, y'know, what did you mean when you said pivot?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Yes, the moon, the glow, the magical feeling, you did this part- Could I get some painkillers over here, please?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[('I', 'PRP'), ('don', 'VBP'), ('?', '.'), ('t', 'NN'), ('want', 'VBP'), ('him', 'PRP'), ('to', 'TO'), ('tell', 'VB'), ('this', 'DT'), ('story', 'NN'), ('for', 'IN'), ('years', 'NNS'), ('.', '.')]\n",
            "[('All', 'DT'), ('right', 'NN'), ('!', '.'), ('Go', 'VB'), ('left', 'JJ'), ('!', '.'), ('Go', 'VB'), ('left', 'JJ'), ('!', '.'), ('Go', 'VB'), ('right', 'JJ'), ('!', '.'), ('!', '.'), ('Go', 'VB'), ('right', 'JJ'), ('!', '.'), ('!', '.')]\n",
            "[('So', 'RB'), ('what', 'WP'), ('do', 'VBP'), ('you', 'PRP'), ('say', 'VB'), (',', ','), ('maybe', 'RB'), ('sometime', 'RB'), ('I', 'PRP'), ('hold', 'VBP'), ('your', 'PRP$'), ('gun', 'NN'), ('?', '.')]\n",
            "[('You', 'PRP'), ('can', 'MD'), ('always', 'RB'), ('spot', 'VB'), ('someone', 'NN'), ('who', 'WP'), (\"'s\", 'VBZ'), ('never', 'RB'), ('seen', 'VBN'), ('one', 'CD'), ('of', 'IN'), ('his', 'PRP$'), ('plays', 'NNS'), ('before', 'IN'), ('.', '.'), ('Notice', 'NNP'), (',', ','), ('no', 'DT'), ('fear', 'NN'), (',', ','), ('no', 'DT'), ('sense', 'NN'), ('of', 'IN'), ('impending', 'VBG'), ('doom', 'NN'), ('...', ':')]\n",
            "[('I', 'PRP'), ('got', 'VBD'), ('caught', 'VBN'), ('up', 'RP'), ('and', 'CC'), ('work', 'NN'), (',', ','), ('but', 'CC'), ('I', 'PRP'), (\"'m\", 'VBP'), ('quitting', 'VBG'), ('tomorrow', 'NN'), ('.', '.')]\n",
            "[('Oh', 'UH'), (',', ','), ('why', 'WRB'), ('not', 'RB'), ('.', '.'), ('Was', 'NNP'), ('I', 'PRP'), ('doing', 'VBG'), ('anything', 'NN'), ('particularly', 'RB'), ('...', ':'), ('saucy', 'NN'), ('?', '.')]\n",
            "[('No', 'DT'), (',', ','), ('I', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('see', 'VB'), ('anything', 'NN'), ('different', 'JJ'), ('other', 'JJ'), ('than', 'IN'), ('the', 'DT'), ('fact', 'NN'), ('that', 'IN'), ('the', 'DT'), ('room', 'NN'), ('got', 'VBD'), ('so', 'RB'), ('much', 'JJ'), ('brighter', 'NN'), ('when', 'WRB'), ('you', 'PRP'), ('came', 'VBD'), ('into', 'IN'), ('it', 'PRP'), ('.', '.')]\n",
            "[('Yeah', 'UH'), (',', ','), ('I', 'PRP'), ('miss', 'VBP'), ('that', 'DT'), ('too', 'RB'), ('.', '.'), ('I', 'PRP'), ('tell', 'VBP'), ('you', 'PRP'), ('what', 'WP'), (';', ':'), ('from', 'IN'), ('now', 'RB'), ('on', 'IN'), ('we', 'PRP'), ('?', '.'), ('ll', 'VB'), ('make', 'JJ'), ('time', 'NN'), ('to', 'TO'), ('hang', 'VB'), ('out', 'RP'), ('with', 'IN'), ('each', 'DT'), ('other', 'JJ'), ('.', '.')]\n",
            "[('I', 'PRP'), ('really', 'RB'), ('shouldn', 'VB'), ('?', '.'), ('t', 'NNS'), ('have', 'VBP'), ('said', 'VBD'), ('that', 'IN'), ('you', 'PRP'), ('were', 'VBD'), ('embarrassing', 'VBG'), ('me', 'PRP'), (',', ','), ('I', 'PRP'), ('mean', 'VBP'), ('that', 'IN'), ('really', 'RB'), ('wasn', 'VB'), ('?', '.'), ('t', 'JJ'), ('cool', 'NN'), ('.', '.')]\n",
            "[('And', 'CC'), ('if', 'IN'), ('it', 'PRP'), ('makes', 'VBZ'), ('you', 'PRP'), ('feel', 'VBP'), ('any', 'DT'), ('better', 'JJR'), (',', ','), ('I', 'PRP'), ('?', '.'), ('ve', 'NN'), ('had', 'VBD'), ('a', 'DT'), ('really', 'RB'), ('lousy', 'JJ'), ('day', 'NN'), ('.', '.')]\n",
            "[('Oh', 'UH'), (',', ','), (\"y'know\", 'UH'), (',', ','), ('what', 'WP'), ('did', 'VBD'), ('you', 'PRP'), ('mean', 'VB'), ('when', 'WRB'), ('you', 'PRP'), ('said', 'VBD'), ('pivot', 'NN'), ('?', '.')]\n",
            "[('Yes', 'UH'), (',', ','), ('the', 'DT'), ('moon', 'NN'), (',', ','), ('the', 'DT'), ('glow', 'NN'), (',', ','), ('the', 'DT'), ('magical', 'JJ'), ('feeling', 'NN'), (',', ','), ('you', 'PRP'), ('did', 'VBD'), ('this', 'DT'), ('part-', 'NN'), ('Could', 'NNP'), ('I', 'PRP'), ('get', 'VBP'), ('some', 'DT'), ('painkillers', 'NNS'), ('over', 'IN'), ('here', 'RB'), (',', ','), ('please', 'VB'), ('?', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFNyDqhJyJyp"
      },
      "source": [
        "4) Joey"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcdXDOchyKBn",
        "outputId": "3021cd6c-b61f-4e8b-921d-dcae5ae84b61"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "\r\n",
        "sentences=\"Then you gotta come clean with Ma! This is not right!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Listen, uh, what do you say I buy you that cup of coffee now?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Whoa-whoa! No-no-no-no-no, nothing is going up! Okay? Up, up is not an option?what's a urethra?  Are you crazy?!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Jo?s there, but I don?t think there?s anything she could do.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Oh thanks. Thanks. It was great meetin? ya. And listen if any of my friends gets married, or have a birthday, or a Tuesday\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Well, Chandler's old roomate was Jewish, and these are the only candles we have, so... Happy Chanukah, everyone.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Vell, Eva, ve've done some excellent vork here, and I vould have to say, your pwoblem is qviiite clear.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Oh sure?And hey, don?t get me wrong, I am so happy for you guys. I just?I miss?hanging out?just-just us, y?know?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"He seemed like a stand up guy. Oh, and he?s not into anything weird sexually.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"All right well, y?know?I guess we know what we have to do to get down.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Ross. I was thinking we could just go down the fire escape.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Noo!! I?ve had the best day ever! Dude, check this out!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Great story!? But, I uh, I gotta go, I got a date with Andrea--Angela--Andrea...? Oh man,\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[('Then', 'RB'), ('you', 'PRP'), ('got', 'VBD'), ('ta', 'JJ'), ('come', 'JJ'), ('clean', 'NN'), ('with', 'IN'), ('Ma', 'NNP'), ('!', '.'), ('This', 'DT'), ('is', 'VBZ'), ('not', 'RB'), ('right', 'JJ'), ('!', '.')]\n",
            "[('Listen', 'NNP'), (',', ','), ('uh', 'UH'), (',', ','), ('what', 'WP'), ('do', 'VBP'), ('you', 'PRP'), ('say', 'VBP'), ('I', 'PRP'), ('buy', 'VBP'), ('you', 'PRP'), ('that', 'IN'), ('cup', 'NN'), ('of', 'IN'), ('coffee', 'NN'), ('now', 'RB'), ('?', '.')]\n",
            "[('Whoa-whoa', 'JJ'), ('!', '.'), ('No-no-no-no-no', 'NN'), (',', ','), ('nothing', 'NN'), ('is', 'VBZ'), ('going', 'VBG'), ('up', 'RP'), ('!', '.'), ('Okay', 'NN'), ('?', '.'), ('Up', 'UH'), (',', ','), ('up', 'RB'), ('is', 'VBZ'), ('not', 'RB'), ('an', 'DT'), ('option', 'NN'), ('?', '.'), ('what', 'WP'), (\"'s\", 'VBZ'), ('a', 'DT'), ('urethra', 'NN'), ('?', '.'), ('Are', 'NNP'), ('you', 'PRP'), ('crazy', 'JJ'), ('?', '.'), ('!', '.')]\n",
            "[('Jo', 'NNP'), ('?', '.'), ('s', 'NN'), ('there', 'RB'), (',', ','), ('but', 'CC'), ('I', 'PRP'), ('don', 'VBP'), ('?', '.'), ('t', 'JJ'), ('think', 'NN'), ('there', 'EX'), ('?', '.'), ('s', 'NN'), ('anything', 'NN'), ('she', 'PRP'), ('could', 'MD'), ('do', 'VB'), ('.', '.')]\n",
            "[('Oh', 'UH'), ('thanks', 'NNS'), ('.', '.'), ('Thanks', 'NNS'), ('.', '.'), ('It', 'PRP'), ('was', 'VBD'), ('great', 'JJ'), ('meetin', 'NN'), ('?', '.'), ('ya', 'NN'), ('.', '.'), ('And', 'CC'), ('listen', 'VB'), ('if', 'IN'), ('any', 'DT'), ('of', 'IN'), ('my', 'PRP$'), ('friends', 'NNS'), ('gets', 'VBZ'), ('married', 'JJ'), (',', ','), ('or', 'CC'), ('have', 'VBP'), ('a', 'DT'), ('birthday', 'NN'), (',', ','), ('or', 'CC'), ('a', 'DT'), ('Tuesday', 'NNP')]\n",
            "[('Well', 'RB'), (',', ','), ('Chandler', 'NNP'), (\"'s\", 'POS'), ('old', 'JJ'), ('roomate', 'NN'), ('was', 'VBD'), ('Jewish', 'JJ'), (',', ','), ('and', 'CC'), ('these', 'DT'), ('are', 'VBP'), ('the', 'DT'), ('only', 'JJ'), ('candles', 'VBZ'), ('we', 'PRP'), ('have', 'VBP'), (',', ','), ('so', 'RB'), ('...', ':'), ('Happy', 'JJ'), ('Chanukah', 'NNP'), (',', ','), ('everyone', 'NN'), ('.', '.')]\n",
            "[('Vell', 'NNP'), (',', ','), ('Eva', 'NNP'), (',', ','), ('ve', 'NN'), (\"'ve\", 'VBP'), ('done', 'VBN'), ('some', 'DT'), ('excellent', 'JJ'), ('vork', 'NN'), ('here', 'RB'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('vould', 'VBP'), ('have', 'VB'), ('to', 'TO'), ('say', 'VB'), (',', ','), ('your', 'PRP$'), ('pwoblem', 'NN'), ('is', 'VBZ'), ('qviiite', 'JJ'), ('clear', 'JJ'), ('.', '.')]\n",
            "[('Oh', 'UH'), ('sure', 'JJ'), ('?', '.'), ('And', 'CC'), ('hey', 'NN'), (',', ','), ('don', 'VB'), ('?', '.'), ('t', 'NN'), ('get', 'VB'), ('me', 'PRP'), ('wrong', 'JJ'), (',', ','), ('I', 'PRP'), ('am', 'VBP'), ('so', 'RB'), ('happy', 'JJ'), ('for', 'IN'), ('you', 'PRP'), ('guys', 'VBP'), ('.', '.'), ('I', 'PRP'), ('just', 'RB'), ('?', '.'), ('I', 'PRP'), ('miss', 'VBP'), ('?', '.'), ('hanging', 'VBG'), ('out', 'RP'), ('?', '.'), ('just-just', 'JJ'), ('us', 'PRP'), (',', ','), ('y', 'VB'), ('?', '.'), ('know', 'VB'), ('?', '.')]\n",
            "[('He', 'PRP'), ('seemed', 'VBD'), ('like', 'IN'), ('a', 'DT'), ('stand', 'NN'), ('up', 'RP'), ('guy', 'NN'), ('.', '.'), ('Oh', 'UH'), (',', ','), ('and', 'CC'), ('he', 'PRP'), ('?', '.'), ('s', 'VBZ'), ('not', 'RB'), ('into', 'IN'), ('anything', 'NN'), ('weird', 'JJ'), ('sexually', 'RB'), ('.', '.')]\n",
            "[('All', 'DT'), ('right', 'RB'), ('well', 'RB'), (',', ','), ('y', 'PRP'), ('?', '.'), ('know', 'VB'), ('?', '.'), ('I', 'PRP'), ('guess', 'VBP'), ('we', 'PRP'), ('know', 'VBP'), ('what', 'WP'), ('we', 'PRP'), ('have', 'VBP'), ('to', 'TO'), ('do', 'VB'), ('to', 'TO'), ('get', 'VB'), ('down', 'RP'), ('.', '.')]\n",
            "[('Ross', 'NNP'), ('.', '.'), ('I', 'PRP'), ('was', 'VBD'), ('thinking', 'VBG'), ('we', 'PRP'), ('could', 'MD'), ('just', 'RB'), ('go', 'VB'), ('down', 'RP'), ('the', 'DT'), ('fire', 'NN'), ('escape', 'NN'), ('.', '.')]\n",
            "[('Noo', 'NN'), ('!', '.'), ('!', '.'), ('I', 'PRP'), ('?', '.'), ('ve', 'NN'), ('had', 'VBD'), ('the', 'DT'), ('best', 'JJS'), ('day', 'NN'), ('ever', 'RB'), ('!', '.'), ('Dude', 'NNP'), (',', ','), ('check', 'VB'), ('this', 'DT'), ('out', 'RP'), ('!', '.')]\n",
            "[('Great', 'NNP'), ('story', 'NN'), ('!', '.'), ('?', '.'), ('But', 'CC'), (',', ','), ('I', 'PRP'), ('uh', 'VBP'), (',', ','), ('I', 'PRP'), ('got', 'VBD'), ('ta', 'RB'), ('go', 'VB'), (',', ','), ('I', 'PRP'), ('got', 'VBD'), ('a', 'DT'), ('date', 'NN'), ('with', 'IN'), ('Andrea', 'NNP'), ('--', ':'), ('Angela', 'NNP'), ('--', ':'), ('Andrea', 'VBP'), ('...', ':'), ('?', '.'), ('Oh', 'UH'), ('man', 'NN'), (',', ',')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Redd14wCye3V"
      },
      "source": [
        "5) Ross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "towZH3ZwyfEo",
        "outputId": "df0f2d93-9b97-4e42-82fc-af3a1d7276ad"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "\r\n",
        "sentences=\"Oh, but he will. He still tells the story how Monica tried to escape from fat camp.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Okay, Andre should be there in like 45 minutes. All rightie, bye bye.  Just easier that way.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"The data we are receiving from MRI scans and DNA testing of these fossils are - are staggering.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"I mean, we've been accepting Leakey's dates as a given, but if they're off by even a hundred thousand years or so then you can - you can just throw most of our assumptions, you know, right in the trash.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"So-so what I am saying is - is is that  is that the repercussions could be huge!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"I mean, not just in palaeontology, but if-if you think about it, in evolutionary biology, uh, genetics, geology, uh, I mean, truly the mind boggles!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"You're right, you're right, it is...So you gonna invite us all to the big opening?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Oh, we were helping Chandler write his vows, but he kicked us out because Joey kept making inappropriate suggestions.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Yeah, laugh all you want but in ten minutes we?re gonna have younger looking skin!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Yeah, I guess we don?t have a choice.  Help us! Please help us! We?re stuck up on the roof and we can?t get down!!!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"I know, I wasn?t finished.   But don?t worry! We?re gonna go down the fire escape!!\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Y'know, here's the thing. Even if I could get it together enough to- to ask a woman out,... who am I gonna ask?\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"Okay, and oh I?m gonna need a bunch of extra keys. Apparently I give them away for no reason at all.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)\r\n",
        "\r\n",
        "sentences=\"I remember the moonlight coming through the window- and her face had the most incredible glow.\"\r\n",
        "tokens=nltk.word_tokenize(sentences)\r\n",
        "tagged=nltk.pos_tag(tokens)\r\n",
        "print(tagged)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[('Oh', 'UH'), (',', ','), ('but', 'CC'), ('he', 'PRP'), ('will', 'MD'), ('.', '.'), ('He', 'PRP'), ('still', 'RB'), ('tells', 'VBZ'), ('the', 'DT'), ('story', 'NN'), ('how', 'WRB'), ('Monica', 'NNP'), ('tried', 'VBD'), ('to', 'TO'), ('escape', 'VB'), ('from', 'IN'), ('fat', 'JJ'), ('camp', 'NN'), ('.', '.')]\n",
            "[('Okay', 'NNP'), (',', ','), ('Andre', 'NNP'), ('should', 'MD'), ('be', 'VB'), ('there', 'RB'), ('in', 'IN'), ('like', 'IN'), ('45', 'CD'), ('minutes', 'NNS'), ('.', '.'), ('All', 'DT'), ('rightie', 'NN'), (',', ','), ('bye', 'NN'), ('bye', 'NN'), ('.', '.'), ('Just', 'RB'), ('easier', 'JJR'), ('that', 'DT'), ('way', 'NN'), ('.', '.')]\n",
            "[('The', 'DT'), ('data', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('receiving', 'VBG'), ('from', 'IN'), ('MRI', 'NNP'), ('scans', 'NNS'), ('and', 'CC'), ('DNA', 'NNP'), ('testing', 'NN'), ('of', 'IN'), ('these', 'DT'), ('fossils', 'NNS'), ('are', 'VBP'), ('-', ':'), ('are', 'VBP'), ('staggering', 'VBG'), ('.', '.')]\n",
            "[('I', 'PRP'), ('mean', 'VBP'), (',', ','), ('we', 'PRP'), (\"'ve\", 'VBP'), ('been', 'VBN'), ('accepting', 'VBG'), ('Leakey', 'NNP'), (\"'s\", 'POS'), ('dates', 'NNS'), ('as', 'IN'), ('a', 'DT'), ('given', 'VBN'), (',', ','), ('but', 'CC'), ('if', 'IN'), ('they', 'PRP'), (\"'re\", 'VBP'), ('off', 'IN'), ('by', 'IN'), ('even', 'RB'), ('a', 'DT'), ('hundred', 'VBN'), ('thousand', 'CD'), ('years', 'NNS'), ('or', 'CC'), ('so', 'RB'), ('then', 'RB'), ('you', 'PRP'), ('can', 'MD'), ('-', ':'), ('you', 'PRP'), ('can', 'MD'), ('just', 'RB'), ('throw', 'VB'), ('most', 'JJS'), ('of', 'IN'), ('our', 'PRP$'), ('assumptions', 'NNS'), (',', ','), ('you', 'PRP'), ('know', 'VBP'), (',', ','), ('right', 'RB'), ('in', 'IN'), ('the', 'DT'), ('trash', 'NN'), ('.', '.')]\n",
            "[('So-so', 'JJ'), ('what', 'WP'), ('I', 'PRP'), ('am', 'VBP'), ('saying', 'VBG'), ('is', 'VBZ'), ('-', ':'), ('is', 'VBZ'), ('is', 'VBZ'), ('that', 'DT'), ('is', 'VBZ'), ('that', 'IN'), ('the', 'DT'), ('repercussions', 'NNS'), ('could', 'MD'), ('be', 'VB'), ('huge', 'JJ'), ('!', '.')]\n",
            "[('I', 'PRP'), ('mean', 'VBP'), (',', ','), ('not', 'RB'), ('just', 'RB'), ('in', 'IN'), ('palaeontology', 'NN'), (',', ','), ('but', 'CC'), ('if-if', 'JJ'), ('you', 'PRP'), ('think', 'VBP'), ('about', 'IN'), ('it', 'PRP'), (',', ','), ('in', 'IN'), ('evolutionary', 'JJ'), ('biology', 'NN'), (',', ','), ('uh', 'UH'), (',', ','), ('genetics', 'NNS'), (',', ','), ('geology', 'NN'), (',', ','), ('uh', 'UH'), (',', ','), ('I', 'PRP'), ('mean', 'VBP'), (',', ','), ('truly', 'RB'), ('the', 'DT'), ('mind', 'NN'), ('boggles', 'VBZ'), ('!', '.')]\n",
            "[('You', 'PRP'), (\"'re\", 'VBP'), ('right', 'JJ'), (',', ','), ('you', 'PRP'), (\"'re\", 'VBP'), ('right', 'JJ'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('...', ':'), ('So', 'RB'), ('you', 'PRP'), ('gon', 'VBP'), ('na', 'TO'), ('invite', 'VB'), ('us', 'PRP'), ('all', 'DT'), ('to', 'TO'), ('the', 'DT'), ('big', 'JJ'), ('opening', 'NN'), ('?', '.')]\n",
            "[('Oh', 'UH'), (',', ','), ('we', 'PRP'), ('were', 'VBD'), ('helping', 'VBG'), ('Chandler', 'NNP'), ('write', 'VB'), ('his', 'PRP$'), ('vows', 'NNS'), (',', ','), ('but', 'CC'), ('he', 'PRP'), ('kicked', 'VBD'), ('us', 'PRP'), ('out', 'RP'), ('because', 'IN'), ('Joey', 'NNP'), ('kept', 'VBD'), ('making', 'VBG'), ('inappropriate', 'JJ'), ('suggestions', 'NNS'), ('.', '.')]\n",
            "[('Yeah', 'UH'), (',', ','), ('laugh', 'IN'), ('all', 'DT'), ('you', 'PRP'), ('want', 'VBP'), ('but', 'CC'), ('in', 'IN'), ('ten', 'JJ'), ('minutes', 'NNS'), ('we', 'PRP'), ('?', '.'), ('re', 'VB'), ('gon', 'NN'), ('na', 'NNS'), ('have', 'VBP'), ('younger', 'JJR'), ('looking', 'VBG'), ('skin', 'NN'), ('!', '.')]\n",
            "[('Yeah', 'UH'), (',', ','), ('I', 'PRP'), ('guess', 'VBP'), ('we', 'PRP'), ('don', 'VB'), ('?', '.'), ('t', 'NNS'), ('have', 'VBP'), ('a', 'DT'), ('choice', 'NN'), ('.', '.'), ('Help', 'VB'), ('us', 'PRP'), ('!', '.'), ('Please', 'VB'), ('help', 'VB'), ('us', 'PRP'), ('!', '.'), ('We', 'PRP'), ('?', '.'), ('re', 'VB'), ('stuck', 'VBN'), ('up', 'RP'), ('on', 'IN'), ('the', 'DT'), ('roof', 'NN'), ('and', 'CC'), ('we', 'PRP'), ('can', 'MD'), ('?', '.'), ('t', 'VB'), ('get', 'VB'), ('down', 'RP'), ('!', '.'), ('!', '.'), ('!', '.')]\n",
            "[('I', 'PRP'), ('know', 'VBP'), (',', ','), ('I', 'PRP'), ('wasn', 'VBP'), ('?', '.'), ('t', 'NN'), ('finished', 'VBN'), ('.', '.'), ('But', 'CC'), ('don', 'VB'), ('?', '.'), ('t', 'NN'), ('worry', 'NN'), ('!', '.'), ('We', 'PRP'), ('?', '.'), ('re', 'VB'), ('gon', 'NN'), ('na', 'TO'), ('go', 'VB'), ('down', 'RP'), ('the', 'DT'), ('fire', 'NN'), ('escape', 'NN'), ('!', '.'), ('!', '.')]\n",
            "[(\"Y'know\", 'NNP'), (',', ','), ('here', 'RB'), (\"'s\", 'VBZ'), ('the', 'DT'), ('thing', 'NN'), ('.', '.'), ('Even', 'RB'), ('if', 'IN'), ('I', 'PRP'), ('could', 'MD'), ('get', 'VB'), ('it', 'PRP'), ('together', 'RB'), ('enough', 'RB'), ('to-', 'NN'), ('to', 'TO'), ('ask', 'VB'), ('a', 'DT'), ('woman', 'NN'), ('out', 'RP'), (',', ','), ('...', ':'), ('who', 'WP'), ('am', 'VBP'), ('I', 'PRP'), ('gon', 'VBP'), ('na', 'TO'), ('ask', 'VB'), ('?', '.')]\n",
            "[('Okay', 'NNP'), (',', ','), ('and', 'CC'), ('oh', 'UH'), ('I', 'PRP'), ('?', '.'), ('m', \"''\"), ('gon', 'NN'), ('na', 'TO'), ('need', 'VB'), ('a', 'DT'), ('bunch', 'NN'), ('of', 'IN'), ('extra', 'JJ'), ('keys', 'NNS'), ('.', '.'), ('Apparently', 'RB'), ('I', 'PRP'), ('give', 'VBP'), ('them', 'PRP'), ('away', 'RB'), ('for', 'IN'), ('no', 'DT'), ('reason', 'NN'), ('at', 'IN'), ('all', 'DT'), ('.', '.')]\n",
            "[('I', 'PRP'), ('remember', 'VBP'), ('the', 'DT'), ('moonlight', 'NN'), ('coming', 'VBG'), ('through', 'IN'), ('the', 'DT'), ('window-', 'JJ'), ('and', 'CC'), ('her', 'PRP$'), ('face', 'NN'), ('had', 'VBD'), ('the', 'DT'), ('most', 'RBS'), ('incredible', 'JJ'), ('glow', 'NN'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b28W3K8Pvbct"
      },
      "source": [
        "##3. 개체명 인식\n",
        "\n",
        "컴퓨터가 사람이름,장소,지역,조직체를 인식할 수 있을까요?\n",
        "\n",
        "즉 Tom lives in NewYork이라는 문장이 있을 때 Tom이 사람이고 NewYork이 장소다는 것을 컴퓨터가 알아차릴 수 있을까요?\n",
        "\n",
        "그것을 가능하게 하는 것이 바로 NER(Name Entity Recogniton)즉 개체명 인식입니다.\n",
        "\n",
        "####Named Entity (개체명) : 사람, 조직, 장소 이름 등 이름을 가진 개체\n",
        "\n",
        "####Name Entity Recognition (개체명 인식) : 텍스트에서 개체명을 인식하고, 그 유형을 알려줌\n",
        "<br>\n",
        "\n",
        "개체명인식을 진행하기 위해선 input 데이터로 형태소태깅 된 리스트가 들어와야 됩니다.\n",
        " \n",
        "즉 개체명인식을 진행하기 위해 먼저 nltk.word_tokenize를 진행하고 nltk.pos_tag를 진행한 후 이것을 input으로 넣어야 함을 알 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHoDfJu2v_po",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9528fa89-6717-41ce-924a-7a2e987e52cc"
      },
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "sentences=\"Oh, but he will. He still tells the story how Monica tried to escape from fat camp.\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "entities=nltk.chunk.ne_chunk(tagged)\n",
        "print(entities)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "(S\n",
            "  Oh/UH\n",
            "  ,/,\n",
            "  but/CC\n",
            "  he/PRP\n",
            "  will/MD\n",
            "  ./.\n",
            "  He/PRP\n",
            "  still/RB\n",
            "  tells/VBZ\n",
            "  the/DT\n",
            "  story/NN\n",
            "  how/WRB\n",
            "  (PERSON Monica/NNP)\n",
            "  tried/VBD\n",
            "  to/TO\n",
            "  escape/VB\n",
            "  from/IN\n",
            "  fat/JJ\n",
            "  camp/NN\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c2wgVaxwV70"
      },
      "source": [
        "## Edit Distance\n",
        "\n",
        "NLTK를 통하여 Edit Distance에 대해서 살펴보려고 합니다.\n",
        "\n",
        "간단히 생각해 2개의 단어가 얼마나 다르냐를 숫자로 표현해주는 것이 Edit Distance입니다.\n",
        "\n",
        " \n",
        "\n",
        "예를 들어 CAT 과  HAT 두단어의 차이는 각 단어의 첫글자인 C와H입니다.\n",
        "\n",
        "즉 1개의 문자만이 차이가 나죠!!\n",
        "\n",
        "따라서 CAT과 HAT의 Edit Distance는 1 이되는 것 입니다.\n",
        "\n",
        "Edit Distance에는 3가지 연산이 있습니다.\n",
        "\n",
        "\n",
        "### Insertion\n",
        "### Deletion\n",
        "### Substitution\n",
        "\n",
        "​\n",
        "\n",
        "즉 삽입, 삭제, 교체 총 이렇게 3가지 연산이 존재합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwtOuNCywx6z"
      },
      "source": [
        "import nltk \n",
        "from nltk.metrics import edit_distance \n",
        "\n",
        "print(edit_distance(\"CAT\",\"HAT\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEBQcfqMwzk1"
      },
      "source": [
        "## Stemming\n",
        "stem이라는 영어단어 들어보셨나요?\n",
        "\n",
        "(식물의) 줄기라는 뜻을 가진 영어단어 인데요.언어학에서는 stem을 어간이라고 합니다.\n",
        "\n",
        "어간은 굴절하는 단어에서 변화하지 않는 부분을 의미합니다.\n",
        "\n",
        "###스태밍(Stemming)이란 어간추출을 의미합니다 !!\n",
        "\n",
        "쉽게 말해 형태가 변한 단어로부터 군더더기를 제거하고 그 단어의 원래 모습을 추출하는 것을 말합니다.\n",
        "\n",
        "예를 들어 going이라는 단어가 있다면 Stemming을 진행할시 go\n",
        "\n",
        "Computers라는 단어를 Stemming을 진행할 시 Comput 를 추출하는 과정을 Stemming이라고 합니다 .\n",
        "\n",
        "\n",
        "포터 스태밍 알고리즘(Porter Stemming Algorithm) 은 Stemming에서 가장 유명한 알고리즘입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRbW6Xv9xOzi"
      },
      "source": [
        "import nltk \n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "pst=PorterStemmer()\n",
        "\n",
        "print(pst.stem(\"computers\"))\n",
        "print(pst.stem(\"going\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIc7CdfRxSYM"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "Lemmatization이란 문장 속에서 다양한 형태로 활용된(inflected) 단어의 표제어(lemma)를 찾는 일을 뜻합니다. <br> \n",
        "여기서 말하는 표제어란 사전에서 단어의 뜻을 찾을 때 쓰는 기본형이라고 생각하면 됩니다.\n",
        "\n",
        "즉 Lemmatization은 단어의 원형을 추출해주는 녀석이라고 생각하면 이해하기 쉽습니다.\n",
        "\n",
        "\n",
        "예를들어 is를 Lemmatization하면 be가 되고 ate을 Lemmatization하면 eat이 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKxKMKpsxhJs"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "wlem=WordNetLemmatizer()\n",
        "\n",
        "print(wlem.lemmatize(\"ate\",pos='v'))\n",
        "print(wlem.lemmatize(\"is\",pos='v'))\n",
        "print(wlem.lemmatize(\"are\",pos='v'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0TOQb8txnLH"
      },
      "source": [
        "## Stopword\n",
        "\n",
        "불용어는 인터넷 검색 시 검색 용어로 사용하지 않는 단어. 관사, 전치사, 조사, 접속사 등 검색 색인 단어로 의미가 없는 단어입니다.\n",
        "\n",
        "\n",
        "{'above', 'doing', 'too', 'can', 'd', 't', 'then', 'what', 'same', 'himself', 'but', 'with', 'on', 'when', 'so', 'isn', 'his', 'further', 'been', 'being', 'our', 'because', 'are', 'from', 'mustn', 'at', 'between', 'here', 'most', 'ours', 'again', 'shouldn', 'have', 'both', 'below', 'against', 'few', 'wasn', 'those', 'hadn', 'once', 'don', 'ain', 'for', 'under', 'o', 're', 'yourselves', 'them', 'themselves', 've', 'about', 'your', 'ourselves', 'who', 'after', 'or', 'he', 'over', 'this', 'how', 'myself', 'into', 'in', 'such', 'aren', 'hasn', 'before', 'whom', 'won', 's', 'were', 'only', 'herself', 'we', 'that', 'was', 'had', 'no', 'of', 'during', 'down', 'has', 'off', 'while', 'where', 'a', 'if', 'until', 'weren', 'be', 'having', 'theirs', 'doesn', 'will', 'to', 'just', 'her', 'ma', 'll', 'there', 'and', 'does', 'other', 'their', 'own', 'why', 'itself', 'its', 'each', 'by', 'not', 'she', 'some', 'him', 'very', 'm', 'should', 'now', 'couldn', 'yourself', 'these', 'as', 'didn', 'an', 'nor', 'is', 'yours', 'did', 'the', 'do', 'my', 'all', 'needn', 'y', 'which', 'up', 'shan', 'haven', 'through', 'me', 'out', 'mightn', 'wouldn', 'they', 'i', 'you', 'hers', 'it', 'more', 'any', 'am', 'than'}\n",
        "\n",
        " \n",
        "\n",
        "영어에 불용어의 종류로는 이렇게 많은 단어들이 있으며 대부분 검색 시 의미없는 단어들입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p01BcNqUxuz0"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop=set(stopwords.words('english'))\n",
        "print(stop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TOCpR5Jx4Xt"
      },
      "source": [
        "## 문장이 주어졌을 때 불용어를 제외한 단어들만 추출하는 방법\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPY5Sxdox6kF"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "stop=set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "sen=\"I want to go to shopping and a I want to buy some of snack\"\n",
        "tokens=nltk.word_tokenize(sen)\n",
        "\n",
        "#clean_tokens=[tok for tok in tokens if len(tok.lower())>1 and (tok.lower() not in stop)]\n",
        "\n",
        "clean_tokens=[]\n",
        "for tok in tokens:\n",
        "  if len(tok.lower())>1 and (tok.lower() not in stop):\n",
        "    clean_tokens.append(tok)\n",
        "\n",
        "  \n",
        "print(\"불용어 포함: \",tokens) \n",
        "print(\"불용어 미포함: \",clean_tokens)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi8iE-vtyAUw"
      },
      "source": [
        "## 특정한 품사만 추출해보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUJXau5HyDiy"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sen=\"Barack Hussein Obama II is an American politician who served as the 44th President of the United States from 2009 to 2017.\"\n",
        "\n",
        "tokens=nltk.word_tokenize(sen)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "\n",
        "#명사추출\n",
        "#allnoun = [word for word, pos in tagged if pos in ['NN', 'NNP'] ]\n",
        "\n",
        "allnoun=[]\n",
        "for word, pos in tagged:\n",
        "  if pos in ['NN', 'NNP']:\n",
        "    allnoun.append(word)\n",
        "    \n",
        "print(\"형태소 분석 결과: \",tagged)\n",
        "print(\"명사만 추출: \", allnoun)\n",
        "\n",
        "#동사 추출\n",
        "#allverb = [word for word, pos in tagged if pos in ['VBZ','VBD'] ]\n",
        "\n",
        "allverb=[]\n",
        "for word, pos in tagged:\n",
        "  if pos in ['VBZ','VBD']:\n",
        "    allverb.append(word)\n",
        "\n",
        "print(\"동사만 추출\",allverb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD9YgojCyMWQ"
      },
      "source": [
        "## HTML 크롤링 후 단어 빈도수 구해보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCDsEi0gyQE2"
      },
      "source": [
        "import nltk\n",
        "import urllib\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords  \n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "#nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "response=urllib.request.urlopen('http://python.org/') #웹에 정보를 요청한 후, 돌려받은 응답을 저장하여 ‘응답 객체(HTTPResponse)’를 반환한다.\n",
        "html=response.read()\n",
        "clean=BeautifulSoup(html,'html.parser').get_text() #html 코드 정제 진행\n",
        "\n",
        "tokens=[]\n",
        "for tok in clean.split():\n",
        "  tokens.append(tok) #하나의 리스트로\n",
        "\n",
        "stop=set(stopwords.words('english')) #불용어 \n",
        "\n",
        "clean_tokens=[]\n",
        "for tok in tokens:\n",
        "  if len(tok.lower())>1 and (tok.lower() not in stop): #길이가 1 이상 인 것 !! stop word가 아닌 것 !\n",
        "    clean_tokens.append(tok)\n",
        "\n",
        "Freq_dist_nltk=nltk.FreqDist(clean_tokens) #FreqDist 클래스는 문서에 사용된 단어(토큰)의 사용빈도 정보를 담는 클래스이다.\n",
        "Freq_dist_nltk.plot(30, cumulative=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RNAKmq9yc_t"
      },
      "source": [
        "## 명사만 추출해보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB5eNascyb0M"
      },
      "source": [
        "import nltk\n",
        "import urllib\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords  \n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "response=urllib.request.urlopen('http://python.org/') \n",
        "html=response.read()\n",
        "clean=BeautifulSoup(html,'html.parser').get_text()   \n",
        "\n",
        "tokens=[tok for tok in clean.split()]\n",
        "\n",
        "stop=set(stopwords.words('english')) \n",
        "\n",
        "clean_tokens= [tok for tok in tokens if len(tok.lower())>1 and (tok.lower() not in stop)] \n",
        "\n",
        "tagged=nltk.pos_tag(clean_tokens)\n",
        "\n",
        "allnoun=[]\n",
        "for word,pos in tagged:\n",
        "  if pos in ['NN','NNP']:\n",
        "    allnoun.append(word)\n",
        "    \n",
        "Freq_dist_nltk = nltk.FreqDist(allnoun) \n",
        "Freq_dist_nltk.plot(30, cumulative=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai5LijaLvDG_"
      },
      "source": [
        "## 한국어 형태소분석"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUmgKgMQvC3e"
      },
      "source": [
        "# konlpy 패키지 다운로드\n",
        "# Error 발생 시 다시 실행\n",
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NskKEBhfvLA2"
      },
      "source": [
        "# konlpy 관련 패키지 import\n",
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma\n",
        "from konlpy.tag import Hannanum\n",
        "from konlpy.tag import Komoran\n",
        "from konlpy.tag import Twitter\n",
        "\n",
        "kkma = Kkma()\n",
        "okt = Okt()\n",
        "komoran = Komoran()\n",
        "hannanum = Hannanum()\n",
        "twitter = Twitter()\n",
        "\n",
        "\n",
        "# konlpy 중 Kkma는 문장 분리가 가능 (다른 라이브러리는 되지 않음)\n",
        "print (\"kkma 문장 분리 : \", kkma.sentences('네 안녕하세요 반갑습니다.'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELnqvwEOvTnG"
      },
      "source": [
        "# konlpy 의 라이브러리 형태소 분석 비교\n",
        "print(\"okt 형태소 분석 :\", okt.morphs(u\"집에 가면 감자 좀 쪄줄래?\"))\n",
        "print(\"kkma 형태소 분석 : \", kkma.morphs(u\"집에 가면 감자 좀 쪄줄래?\"))\n",
        "print(\"hannanum 형태소 분석 : \", hannanum.morphs(u\"집에 가면 감자 좀 쪄줄래?\"))\n",
        "print(\"komoran 형태소 분석 : \", komoran.morphs(u\"집에 가면 감자 좀 쪄줄래?\"))\n",
        "print(\"twitter 형태소 분석 : \", twitter.morphs(u\"집에 가면 감자 좀 쪄줄래?\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7cp8u2lwpDx"
      },
      "source": [
        "# konlpy 의 라이브러리 품사태깅 비교\n",
        "print(\"okt 품사태깅 :\", okt.pos(u\"집에 가면 감자 좀 쪄줄래?\"))\n",
        "print(\"kkma 품사태깅 : \", kkma.pos(u\"집에 가면 감자 좀 쪄줄래?\"))\n",
        "print(\"hannanum 품사태깅 : \", hannanum.pos(u\"집에 가면 감자 좀 쪄줄래?\"))\n",
        "print(\"komoran 품사태깅 : \", komoran.pos(u\"집에 가면 감자 좀 쪄줄래?\"))\n",
        "print(\"twitter 품사태깅 : \", twitter.pos(u\"집에 가면 감자 좀 쪄줄래?\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}